{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Notes on Variational Autoencoders","text":""},{"location":"#about-the-book","title":"About the Book","text":"<p>These notes on basic VAE architecture and extensions. We somewhat follow the notes:</p> <p>Dynamical Variational Autoencoders: A Comprehensive Review  Laurent Girin, Simon Leglaive, Xiaoyu Bie, Julien Diard</p> <p>We use GPT-o1 and o4-mini for the Q&amp;A which make up these notes.</p>"},{"location":"#chapters-overview-for-sidebar-on-home-page","title":"Chapters Overview (for sidebar on Home page)","text":"<ul> <li>Chapter 1: Basic VAE</li> <li>Chapter 2: Extension to Tranformer encoder</li> </ul>"},{"location":"chapter1/","title":"Chapter 1: Reinforcement Learning basics","text":""},{"location":"chapter1/#vae-architecture-1","title":"VAE Architecture 1","text":"<p>Let \\(y_k\\) be measurements of a data generating process. I would like to use the Variational Autoencoder method of statistical inference, to learn about the data generating process. Describe the details of the VAE to accomplish this.</p> <p>Answer</p> <p>Below is a self-contained description of how to set up and train a Variational Autoencoder (VAE) to infer the data-generating process behind measurements \\(y_k\\).  All formulas are in standard LaTeX.</p> <p>1 Model specification    We introduce a low-dimensional latent variable \\(z\\in\\mathbb R^d\\).  The generative model (decoder) and the inference model (encoder) are</p> \\[      \\text{Prior:}\\quad p(z)=\\mathcal N\\bigl(z;0,I\\bigr),      \\quad      \\text{Likelihood:}\\quad p_\\theta(y\\mid z), \\] \\[      \\text{Approx.\\ posterior:}\\quad      q_\\phi(z\\mid y)      =\\mathcal N\\bigl(z;\\,\\mu_\\phi(y),\\,\\mathrm{diag}(\\sigma_\\phi^2(y))\\bigr). \\] <p>Here \\(\\theta\\) are the parameters of the decoder network (which outputs parameters of \\(p_\\theta(y\\mid z)\\)), and \\(\\phi\\) are the parameters of the encoder network (which outputs \\(\\mu_\\phi(y)\\) and \\(\\sigma_\\phi(y)\\)).</p> <p>2 Evidence Lower BOund (ELBO)    For a single datum \\(y\\), the marginal log-likelihood \\(\\log p_\\theta(y)\\) is intractable.  We instead maximize the ELBO</p> \\[      \\mathcal L(\\theta,\\phi\\,;\\,y)      \\;=\\;      \\mathbb{E}_{q_\\phi(z\\mid y)}\\bigl[\\log p_\\theta(y\\mid z)\\bigr]      \\;-\\;      D_{\\mathrm{KL}}\\bigl(q_\\phi(z\\mid y)\\;\\|\\;p(z)\\bigr), \\] <p>which satisfies \\(\\log p_\\theta(y)\\ge\\mathcal L(\\theta,\\phi;y)\\).  Over a dataset \\(\\{y_k\\}_{k=1}^N\\) we maximize</p> \\[      \\mathcal L_{\\rm total}(\\theta,\\phi)      = \\sum_{k=1}^N \\mathcal L(\\theta,\\phi\\,;\\,y_k). \\] <p>3 Reparameterization trick    To backpropagate through the stochastic sampling \\(z\\sim q_\\phi(z\\mid y)\\), we write</p> \\[      z \\;=\\;\\mu_\\phi(y)\\;+\\;\\sigma_\\phi(y)\\odot\\epsilon,      \\quad      \\epsilon\\sim\\mathcal N(0,I). \\] <p>Then</p> \\[      \\mathbb{E}_{q_\\phi(z\\mid y)}\\bigl[\\log p_\\theta(y\\mid z)\\bigr]      \\;=\\;      \\mathbb{E}_{\\epsilon\\sim\\mathcal N(0,I)}      \\bigl[\\log p_\\theta\\bigl(y\\mid \\mu_\\phi(y)+\\sigma_\\phi(y)\\odot\\epsilon\\bigr)\\bigr], \\] <p>which can be unbiasedly estimated with Monte Carlo and differentiated w.r.t.\\ \\(\\phi\\).</p> <p>4 KL-divergence term    For two Gaussians \\(q=\\mathcal N(\\mu,\\mathrm{diag}(\\sigma^2))\\) and \\(p=\\mathcal N(0,I)\\),</p> \\[      D_{\\rm KL}\\bigl(q\\|p\\bigr)      =\\frac12\\sum_{i=1}^d        \\bigl[          \\sigma_i^2+\\mu_i^2-1-\\log(\\sigma_i^2)        \\bigr]. \\] <p>5 Overall training algorithm    Use mini-batches \\(\\{y^{(b)}\\}\\).  For each batch:    a) Encode: compute \\(\\mu_\\phi(y^{(b)})\\), \\(\\sigma_\\phi(y^{(b)})\\).    b) Sample \\(\\epsilon\\sim\\mathcal N(0,I)\\), form \\(z=\\mu_\\phi+\\sigma_\\phi\\odot\\epsilon\\).    c) Decode: compute \\(\\log p_\\theta(y^{(b)}\\mid z)\\).    d) Compute batch ELBO </p> \\[         \\mathcal L_{\\rm batch}          =\\frac1B\\sum_{i=1}^B           \\Bigl[\\log p_\\theta(y^{(i)}\\!\\mid z^{(i)})             -D_{\\rm KL}(q_\\phi(z^{(i)}\\mid y^{(i)})\\|p(z))\\Bigr]. \\] <p>e) Take gradient \\(\\nabla_{\\theta,\\phi}\\mathcal L_{\\rm batch}\\) (e.g.\\ via Adam) and update \\(\\theta,\\phi\\).</p> <p>6 Choice of decoders    \u2013 For real-valued \\(y\\): use Gaussian likelihood \\(\\;p_\\theta(y\\mid z)=\\mathcal N(y;\\,\\mu_\\theta(z),\\,\\mathrm{diag}(\\sigma_y^2))\\).    \u2013 For binary \\(y\\): use Bernoulli \\(\\;p_\\theta(y\\mid z)=\\mathrm{Bernoulli}(y;\\,\\pi_\\theta(z)).\\)</p> <p>7 Inference about the data-generating process    After training, the decoder \\(p_\\theta(y\\mid z)\\) and the aggregated posterior give you a generative model of the data.  You can    \u2022 Sample new synthetic observations \\(y\\sim p_\\theta(y\\mid z)\\) by first drawing \\(z\\sim p(z)\\).    \u2022 Approximate the latent representation of any \\(y\\) by \\(q_\\phi(z\\mid y)\\).    \u2022 Inspect learned features by varying individual components of \\(z\\) and decoding.</p> <p>This completes the detailed VAE setup for learning the underlying data-generating process from measurements \\(y_k\\).</p>"},{"location":"chapter1/#vae-architecture-2-encoder","title":"VAE Architecture 2 - Encoder","text":"<p>Let\u2019s make the setting more precise: the measurements \\(y_k \\in \\mathbb{R}^n\\) and the index \\(k\\). Let \\(Y \\in \\mathbb{R}^n\\) be the random variable (vector) and the \\(y_k\\) are the observations or realizations of it. </p> <p>The encoder takes as input the \\(y_k\\)  and models a probability distribution \\(q_{\\phi}(z | y)\\). Thus, for each \\((k, y_k)\\), the encoder introduces a \\(z_k\\). Is this understanding correct so far?</p> <p>Answer</p> <p>Yes\u2014your understanding is exactly right.  In a VAE we introduce for each observation \\(y_k\\) a corresponding latent variable \\(z_k\\), and the encoder network \u201camortizes\u201d inference by outputting an approximate posterior over \\(z\\) given \\(y\\).  Concretely, one usually chooses</p> \\[ q_{\\phi}(z\\mid y) =\\mathcal N\\bigl(z;\\,\\mu_{\\phi}(y),\\,\\mathrm{diag}(\\sigma_{\\phi}^2(y))\\bigr), \\] <p>so that for each data point \\(y_k\\) you have</p> \\[ q_{\\phi}(z\\mid y_k) =\\mathcal N\\bigl(z;\\,\\mu_{\\phi}(y_k),\\,\\mathrm{diag}(\\sigma_{\\phi}^2(y_k))\\bigr). \\] <p>During training (and inference) you then draw</p> \\[ z_k \\;=\\;\\mu_{\\phi}(y_k)\\;+\\;\\sigma_{\\phi}(y_k)\\odot\\epsilon_k, \\qquad \\epsilon_k\\sim\\mathcal N(0,I), \\] <p>and pass \\(z_k\\) into the decoder \\(p_{\\theta}(y\\mid z)\\).  Thus each \\((k,y_k)\\) is associated with its own latent \\(z_k\\), but the map \\(y\\mapsto(\\mu_{\\phi}(y),\\sigma_{\\phi}(y))\\) is shared across all \\(k\\).</p>"},{"location":"chapter1/#vae-architecture-3-encoder-contd","title":"VAE Architecture 3 - Encoder (contd)","text":"<p>The encoder map \\(y \\mapsto (\\mu_{\\phi}(y), \\sigma_{\\phi}(y))\\) is modeled as a neural network which takes as input the \\(y\\) and outputs \\((\\mu_{\\phi}(y), \\sigma_{\\phi}(y))\\) which are the mean and standard deviation of a Gaussian distribution. So in fact, for each \\(y_k\\), there is a unique Gaussian distribution from which the corresponding \\(z_k\\) is drawn. Is this correct?</p> <p>Answer</p> <p>Yes.  Concretely, the encoder is a deterministic function  </p> \\[ y\\;\\mapsto\\;\\bigl(\\mu_\\phi(y),\\,\\sigma_\\phi(y)\\bigr)\\in\\mathbb R^d\\times\\mathbb R^d_{&gt;0}, \\] <p>so that for each observation \\(y_k\\) you obtain parameters of a Gaussian,  </p> \\[ q_\\phi(z\\mid y_k) =\\mathcal N\\bigl(z;\\,\\mu_\\phi(y_k),\\,\\mathrm{diag}(\\sigma_\\phi(y_k)^2)\\bigr). \\] <p>You then draw  </p> \\[ z_k \\;=\\;\\mu_\\phi(y_k)\\;+\\;\\sigma_\\phi(y_k)\\odot\\epsilon_k, \\quad \\epsilon_k\\sim\\mathcal N(0,I), \\] <p>so indeed each \\(y_k\\) induces its own Gaussian from which \\(z_k\\) is sampled.</p>"},{"location":"chapter1/#sampling-the-latent-variable-z-reparameterization-trick","title":"Sampling the latent variable \\(z\\): Reparameterization trick","text":"<p>It would seem that sampling \\(z_k\\) from the Gaussian directly, that is $ z_k \\sim \\mathcal{N}(z; \\mu_{\\phi}(y_k), diag(\\sigma_{\\phi}(y_k)^2))$ is more natural than in the manner you describe.</p> <p>Answer</p> <p>Your intuition is absolutely right: in the VAE we do want to \u201creally\u201d draw  </p> \\[ z_k \\;\\sim\\;\\mathcal N\\!\\bigl(\\mu_{\\phi}(y_k)\\,,\\,\\mathrm{diag}(\\sigma_{\\phi}(y_k)^2)\\bigr) \\] <p>for each data\u2010point \\(y_k\\).  The reason we write it in the form</p> \\[ z_k \\;=\\;\\mu_{\\phi}(y_k)\\;+\\;\\sigma_{\\phi}(y_k)\\odot\\epsilon_k, \\quad \\epsilon_k\\sim\\mathcal N(0,I) \\] <p>is simply a reparameterization of that same Gaussian draw.  Algebraically the two are equivalent.  The reparameterization is introduced only so that when you back\u2010propagate the reconstruction\u2010loss </p> \\[ \\mathbb{E}_{q_\\phi(z\\mid y_k)}\\bigl[\\log p_\\theta(y_k\\mid z)\\bigr] \\] <p>through the encoder network, you get low\u2010variance, unbiased gradients w.r.t.\\ \\(\\phi\\).  </p> <p>If you were to write  </p> \\[ z_k \\;\\sim\\;\\mathcal N\\bigl(\\mu_{\\phi}(y_k),\\,\\mathrm{diag}(\\sigma_{\\phi}(y_k)^2)\\bigr) \\] <p>and then try to differentiate directly through that sampling operation, the path from \\(\\phi\\) into the sample \\(z_k\\) would be discontinuous and you would have to resort to score\u2010function estimators (REINFORCE), which typically have much higher variance.</p> <p>Thus: 1  Yes, each \\(y_k\\) defines its own Gaussian for \\(z_k\\). 2  Writing the draw via  </p> \\[      z_k = \\mu_\\phi(y_k) + \\sigma_\\phi(y_k)\\odot\\epsilon_k \\] <p>is just a mathematically equivalent way of expressing  </p> \\[      z_k\\sim\\mathcal N\\bigl(\\mu_\\phi(y_k),\\,\\mathrm{diag}(\\sigma_\\phi(y_k)^2)\\bigr) \\] <p>that makes it possible to back\u2010propagate gradients through the random draw.</p>"},{"location":"chapter1/#reparameterization-trick-2","title":"Reparameterization Trick 2","text":"<p>Show the mathematical equivalence of the two ways of drawing \\(z_k\\) rigorously.</p> <p>Answer</p> <p>Below is a standard \u201cchange\u2010of\u2010variables\u201d proof (using the Jacobian) and an alternative proof via moment\u2010generating functions, which together show rigorously that</p> \\[ z=\\mu+\\sigma\\odot\\epsilon,\\quad\\epsilon\\sim\\mathcal N(0,I_d) \\] <p>has exactly the same law as</p> \\[ z\\sim\\mathcal N\\bigl(\\mu,\\;\\mathrm{diag}(\\sigma^2)\\bigr). \\] <p>1  Change\u2010of\u2010variables (Jacobian) proof    Let \\(d\\) be the dimension of \\(z\\).  Write \\(\\sigma=(\\sigma_1,\\dots,\\sigma_d)\\) and define the diagonal matrix</p> <p>$$      \\Sigma=\\mathrm{diag}(\\sigma_1,\\dots,\\sigma_d)\\in\\mathbb R^{d\\times d}.    $$</p> <p>We draw</p> <p>$$      \\epsilon\\sim p_\\epsilon(\\epsilon)      =\\frac1{(2\\pi)^{d/2}}        \\exp!\\Bigl(-\\tfrac12\\,\\epsilon^\\top\\epsilon\\Bigr),      \\qquad      z \\;=\\;\\mu+\\Sigma\\,\\epsilon.    $$</p> <p>Since the map</p> <p>$$      T:\\;\\epsilon\\mapsto z=T(\\epsilon)=\\mu+\\Sigma\\,\\epsilon    $$</p> <p>is invertible with</p> <p>$$      T^{-1}(z)=\\Sigma^{-1}(z-\\mu),      \\quad      \\det\\Bigl(\\tfrac{\\partial T^{-1}}{\\partial z}\\Bigr)      =\\det(\\Sigma^{-1})      =\\prod_{i=1}^d\\frac1{\\sigma_i}\\,,    $$</p> <p>the density of \\(z\\) is</p> \\[      p_z(z)      =p_\\epsilon\\bigl(T^{-1}(z)\\bigr)       \\;\\Bigl|\\det\\bigl(\\tfrac{\\partial T^{-1}}{\\partial z}\\bigr)\\Bigr|      =\\frac1{(2\\pi)^{d/2}}       \\exp\\!\\Bigl(-\\tfrac12\\|\\Sigma^{-1}(z-\\mu)\\|^2\\Bigr)       \\;\\prod_{i=1}^d\\frac1{\\sigma_i}. \\] <p>Noting     \\(\\|\\Sigma^{-1}(z-\\mu)\\|^2=\\sum_i\\frac{(z_i-\\mu_i)^2}{\\sigma_i^2}\\)    and \\(\\prod_i\\sigma_i^{-1}=(\\det\\Sigma)^{-1}\\), this becomes</p> <p>$$      p_z(z)      =\\frac1{(2\\pi)^{d/2}\\,\\prod_i\\sigma_i}       \\exp!\\Bigl(-\\tfrac12\\sum_{i=1}^d\\frac{(z_i-\\mu_i)^2}{\\sigma_i^2}\\Bigr),    $$</p> <p>which is exactly the density of \\(\\mathcal N(\\mu,\\mathrm{diag}(\\sigma^2))\\).</p> <p>2  Moment\u2010Generating\u2010Function (MGF) proof    Recall the MGF of a random vector \\(w\\) is \\(M_w(t)=\\mathbb E[e^{t^\\top w}]\\).  If \\(w\\sim\\mathcal N(0,I)\\), then \\(M_w(t)=\\exp(\\tfrac12\\,t^\\top t)\\).  Now set</p> <p>$$      z=\\mu+\\Sigma\\,\\epsilon,      \\quad      \\epsilon\\sim\\mathcal N(0,I).    $$</p> <p>Then for any \\(t\\in\\mathbb R^d\\),</p> \\[      M_z(t)      =\\mathbb E\\bigl[e^{t^\\top z}\\bigr]      =\\mathbb E\\bigl[e^{\\,t^\\top(\\mu+\\Sigma\\epsilon)}\\bigr]      =e^{\\,t^\\top\\mu}\\;\\mathbb E\\bigl[e^{\\,(t^\\top\\Sigma)\\epsilon}\\bigr]      =e^{\\,t^\\top\\mu}\\;\\exp\\!\\Bigl(\\tfrac12\\,(t^\\top\\Sigma)(t^\\top\\Sigma)^\\top\\Bigr). \\] <p>But \\((t^\\top\\Sigma)(t^\\top\\Sigma)^\\top=t^\\top\\,\\Sigma^2\\,t\\) and \\(\\Sigma^2=\\mathrm{diag}(\\sigma^2)\\).  Hence</p> <p>$$      M_z(t)      =\\exp!\\Bigl(t^\\top\\mu+\\tfrac12\\,t^\\top\\mathrm{diag}(\\sigma^2)\\,t\\Bigr),    $$</p> <p>which is precisely the MGF of \\(\\mathcal N(\\mu,\\mathrm{diag}(\\sigma^2))\\).  By uniqueness of MGFs,  </p> <p>$$      z\\;\\sim\\;\\mathcal N\\bigl(\\mu,\\mathrm{diag}(\\sigma^2)\\bigr).    $$</p> <p>Conclusion: drawing </p> \\[ z_k\\;\\sim\\;\\mathcal N\\bigl(\\mu_\\phi(y_k),\\,\\mathrm{diag}(\\sigma_\\phi(y_k)^2)\\bigr) \\] <p>at the sampling step is mathematically equivalent to writing </p> \\[ z_k=\\mu_\\phi(y_k)+\\sigma_\\phi(y_k)\\odot\\epsilon_k, \\quad \\epsilon_k\\sim\\mathcal N(0,I), \\] <p>since both constructions produce the same Gaussian law for \\(z_k\\).</p>"},{"location":"chapter1/#vae-architecture-4-decoder","title":"VAE Architecture 4 - Decoder","text":"<p>For the decoder, it appears we have a neural network which models  \\(p_\\theta(y\\mid z)=\\mathcal N(y;\\,\\mu_\\theta(z),\\,\\mathrm{diag}(\\sigma_{\\theta}^2(z)))\\). The input to this neural network is \\(z_k\\) and the output are the Gaussian parameters \\(\\mu_{\\theta}(z_k)\\) and \\(\\sigma_{\\theta}(z_k)\\). The reconstruction \\(\\tilde{y}_k\\) is drawn from this Gaussian distribution.</p> <p>Answer</p> <p>Yes, that\u2019s exactly the usual Gaussian\u2010decoder VAE.  Concretely:</p> <p>1 Decoder network    You have a deterministic neural network  </p> \\[      z\\;\\longmapsto\\;\\bigl(\\mu_\\theta(z),\\,\\sigma_\\theta(z)\\bigr)      \\;\\in\\;\\mathbb R^n\\times\\mathbb R^n_{&gt;0}, \\] <p>parameterized by \\(\\theta\\).  Here \\(\\mu_\\theta(z)\\) and \\(\\sigma_\\theta(z)\\) are the per-coordinate means and standard deviations of your Gaussian likelihood.</p> <p>2 Likelihood    Given a latent \\(z_k\\), you define</p> \\[      p_\\theta\\bigl(y\\mid z_k\\bigr)      =\\mathcal N\\!\\bigl(y\\;;\\;\\mu_\\theta(z_k),\\;\\mathrm{diag}(\\sigma_\\theta(z_k)^2)\\bigr). \\] <p>3 Reconstruction sample    To generate (or \u201creconstruct\u201d) a sample \\(\\tilde y_k\\) from the decoder you draw</p> <p>$$      \\tilde y_k      \\;=\\;\\mu_\\theta(z_k)\\;+\\;\\sigma_\\theta(z_k)\\odot\\epsilon,      \\qquad      \\epsilon\\sim\\mathcal N(0,I_n).    $$</p> <p>Equivalently,</p> <p>$$      \\tilde y_k\\;\\sim\\;\\mathcal N!\\bigl(\\mu_\\theta(z_k),\\,\\mathrm{diag}(\\sigma_\\theta(z_k)^2)\\bigr).    $$</p> <p>4 ELBO term    In practice, when computing the reconstruction term of the ELBO you don\u2019t need to draw \\(\\tilde y_k\\) explicitly.  You compute</p> \\[      \\mathbb E_{q_\\phi(z\\mid y_k)}\\Bigl[\\log p_\\theta(y_k\\mid z)\\Bigr]      \\;\\approx\\;      \\frac1L\\sum_{\\ell=1}^{L}      \\log \\mathcal N\\!\\bigl(y_k\\;;\\;\\mu_\\theta(z_k^{(\\ell)}),\\,\\mathrm{diag}(\\sigma_\\theta(z_k^{(\\ell)})^2)\\bigr) \\] <p>using samples \\(z_k^{(\\ell)}\\) from \\(q_\\phi(z\\mid y_k)\\).  For a single sample the Gaussian log-density is</p> \\[      \\log p_\\theta(y_k\\mid z_k)      =-\\frac12\\sum_{i=1}^n\\Bigl[        \\tfrac{\\bigl(y_{k,i}-\\mu_{\\theta,i}(z_k)\\bigr)^2}{\\sigma_{\\theta,i}(z_k)^2}        +\\log\\bigl(2\\pi\\,\\sigma_{\\theta,i}(z_k)^2\\bigr)      \\Bigr]. \\] <p>So yes: the decoder takes \\(z_k\\) as input, outputs \\(\\mu_\\theta(z_k)\\) and \\(\\sigma_\\theta(z_k)\\), and then the reconstruction \\(\\tilde y_k\\) is sampled from that Gaussian.</p>"},{"location":"chapter1/#training","title":"Training","text":"<p>Now that we have clarified the architecture of the VAE, let\u2019s discuss the details of the training. The observation index \\(k = 1, \\cdots, m\\). There are \\(m\\) observations. \\(y_k \\in \\mathbb{R}^n\\).  What is the objective function for training? Clearly there must be a term to minimize the reconstruction error \\((y_k - \\tilde{y}_k)\\). The training phase must result in fixing of the parameters \\(\\phi\\) and \\(\\theta\\) of the encoder and decoder networks. Explain the details of training.</p> <p>Answer</p> <p>The training objective of a VAE is to maximize, over your encoder\u2010parameters \\(\\phi\\) and decoder\u2010parameters \\(\\theta\\), the (empirical) Evidence Lower BOund (ELBO) on the log\u2010likelihood of your \\(m\\) data points \\(\\{y_k\\}_{k=1}^m\\).  Equivalently one minimizes the negative ELBO, which decomposes into a reconstruction term plus a regularizing KL term.  Concretely:</p> <ol> <li> <p>Definition of the ELBO for a single datum     For each observation \\(y_k\\), the ELBO is</p> \\[   \\mathcal L(\\theta,\\phi;\\,y_k)   \\;=\\;   \\underbrace{\\mathbb{E}_{z\\sim q_\\phi(z\\mid y_k)}        \\bigl[\\log p_\\theta(y_k\\mid z)\\bigr]}_{\\displaystyle\\text{(a) reconstruction}}    \\;-\\;   \\underbrace{D_{\\rm KL}\\bigl(q_\\phi(z\\mid y_k)\\,\\|\\,p(z)\\bigr)}_     {\\displaystyle\\text{(b) regularizer}}. \\] <p>Here (a) \\(\\mathbb{E}_{q_\\phi}[\\log p_\\theta(y_k\\mid z)]\\) encourages the decoder to place high likelihood on \\(y_k\\), (b) \\(D_{\\rm KL}(q_\\phi(z\\mid y_k)\\,\\|\\,p(z))\\) pushes the approximate posterior toward the prior \\(p(z)=\\mathbb{N}(0,I)\\).</p> </li> <li> <p>Empirical objective over the dataset     Replace the population expectation by the empirical average over \\(k=1,\\dots,m\\).  The total ELBO is</p> \\[   \\mathcal L_{\\rm total}(\\theta,\\phi)   = \\sum_{k=1}^m \\mathcal L(\\theta,\\phi;\\,y_k)   \\;=\\;   \\sum_{k=1}^m   \\Bigl\\{   \\mathcal{E}_{q_\\phi(z\\mid y_k)}\\bigl[\\log p_\\theta(y_k\\mid z)\\bigr]   \\;-\\;   D_{\\rm KL}\\bigl(q_\\phi(z\\mid y_k)\\,\\|\\,p(z)\\bigr)   \\Bigr\\}. \\] <p>In practice we maximize \\(\\mathcal L_{\\rm total}\\), or equivalently minimize the loss</p> \\[   \\mathcal J(\\theta,\\phi)   = -\\,\\mathcal L_{\\rm total}(\\theta,\\phi). \\] </li> <li> <p>Writing out the two terms     3.1 Reconstruction term     If you use a Gaussian decoder  </p> \\[   p_\\theta(y\\mid z)   = \\mathbb{N}\\!\\bigl(y;\\,\\mu_\\theta(z),\\,\\text{diag}(\\sigma_\\theta(z)^2)\\bigr), \\] <p>then</p> \\[   \\log p_\\theta(y_k\\mid z)   = -\\frac12\\sum_{i=1}^n     \\Bigl[\\tfrac{(y_{k,i}-\\mu_{\\theta,i}(z))^2}{\\sigma_{\\theta,i}(z)^2}           +\\log\\bigl(2\\pi\\,\\sigma_{\\theta,i}(z)^2\\bigr)     \\Bigr]. \\] <p>In many implementations one further fixes \\(\\sigma_{\\theta,i}(z)=1\\), so that \\(\\log p_\\theta(y_k\\mid z)\\propto -\\tfrac12\\|y_k-\\mu_\\theta(z)\\|^2\\), i.e.\\ a mean\u2010squared error.</p> <p>3.2 KL\u2010divergence term Since</p> \\[   q_\\phi(z\\mid y_k)   = \\mathbb{N}\\!\\bigl(z;\\,\\mu_\\phi(y_k),\\,\\text{diag}(\\sigma_\\phi(y_k)^2)\\bigr),   \\quad   p(z)=\\mathbb{N}(0,I), \\] <p>one has the closed\u2010form</p> \\[   D_{\\rm KL}\\bigl(q_\\phi(z\\mid y_k)\\,\\|\\,p(z)\\bigr)   = \\frac12\\sum_{j=1}^d     \\bigl[\\sigma_{\\phi,j}(y_k)^2           +\\mu_{\\phi,j}(y_k)^2           -1           -\\log\\sigma_{\\phi,j}(y_k)^2     \\bigr]. \\] </li> <li> <p>Stochastic optimization via mini\u2010batches     You typically do not compute the full sum over \\(k=1,\\dots,m\\) at each step.  Instead you sample a mini\u2010batch \\(\\{y_{k_1},\\dots,y_{k_B}\\}\\) of size \\(B\\), and approximate the ELBO by</p> \\[   \\widehat{\\mathcal L}_{\\rm batch}   = \\frac1B\\sum_{b=1}^B   \\Bigl\\{     \\log p_\\theta\\bigl(y_{k_b}\\mid z_{k_b}\\bigr)     \\;-\\;     D_{\\rm KL}\\bigl(q_\\phi(z\\mid y_{k_b})\\|\\;p(z)\\bigr)   \\Bigr\\}, \\] <p>where each \\(z_{k_b}\\) is drawn via the reparameterization</p> \\[   z_{k_b}   = \\mu_\\phi\\bigl(y_{k_b}\\bigr) + \\sigma_\\phi\\bigl(y_{k_b}\\bigr)\\odot \\epsilon,\\quad   \\epsilon\\sim\\mathbb{N}(0,I_d). \\] <p>You then take one (or several) gradient steps</p> \\[   \\phi\\;\\leftarrow\\;\\phi + \\alpha\\,\\nabla_\\phi\\,\\widehat{\\mathcal L}_{\\rm batch},   \\qquad   \\theta\\;\\leftarrow\\;\\theta + \\alpha\\,\\nabla_\\theta\\,\\widehat{\\mathcal L}_{\\rm batch}, \\] <p>using, e.g., the Adam optimizer with learning rate \\(\\alpha\\).</p> </li> </ol> <p>5   Summary of the training loop</p> <p>For \\(t=1,\\dots,T\\) (number of gradient steps):</p> <p>a) Sample a mini\u2010batch \\(\\{y_{k_b}\\}_{b=1}^B\\).  b) For each \\(y_{k_b}\\) compute \\(\\mu_\\phi(y_{k_b})\\), \\(\\sigma_\\phi(y_{k_b})\\).  c) Sample \\(\\epsilon_b\\sim\\mathbb{N}(0,I_d)\\), set  </p> \\[             z_{k_b}=\\mu_\\phi(y_{k_b})+\\sigma_\\phi(y_{k_b}) \\odot\\epsilon_b. \\] <p>d) Decode: compute \\(\\mu_\\theta(z_{k_b})\\), \\(\\sigma_\\theta(z_{k_b})\\) and then \\(\\log p_\\theta(y_{k_b}\\mid z_{k_b})\\).  e) Compute the batch ELBO \\(\\widehat{\\mathcal L}_{\\rm batch}\\) and its gradients w.r.t.\\ \\(\\phi,\\theta\\).  f) Update \\(\\phi,\\theta\\) by a gradient\u2010based optimizer.</p> <p>After convergence, you have fixed the encoder parameters \\(\\phi\\) and decoder parameters \\(\\theta\\).  At test time you can (i) encode any new \\(y\\) to \\(q_\\phi(z\\mid y)\\), (ii) sample \\(z\\sim p(z)\\) and decode to generate new data via \\(p_\\theta(y\\mid z)\\).</p>"},{"location":"chapter2/","title":"Getting Started","text":"<p>This chapter covers the basics.</p>"},{"location":"chapter2/#basic-math-examples","title":"Basic Math Examples","text":"<p>Inline math: \\(\\alpha^2 + \\beta^2 = \\gamma^2\\) or \\(E=mc^2\\).</p> <p>Display math:</p> \\[  \\int_0^1 x^2 dx = \\frac{1}{3} \\] <p>Or using LaTeX delimiters:</p> \\[ \\sum_{n=1}^\\infty \\frac{1}{n^2} = \\frac{\\pi^2}{6} \\] <p>Testing 1 2 3</p>"},{"location":"chapter2/#code-snippet","title":"Code Snippet","text":"<p>```python print(\"Hello, World!\")</p>"}]}