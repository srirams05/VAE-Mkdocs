{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Notes on Variational Autoencoders","text":""},{"location":"#about-the-book","title":"About the Book","text":"<p>These notes on basic VAE architecture and extensions. We somewhat follow the notes:</p> <p>Dynamical Variational Autoencoders: A Comprehensive Review  Laurent Girin, Simon Leglaive, Xiaoyu Bie, Julien Diard</p> <p>We use GPT-o1 and o4-mini for the Q&amp;A which make up these notes.</p>"},{"location":"#chapters-overview-for-sidebar-on-home-page","title":"Chapters Overview (for sidebar on Home page)","text":"<ul> <li>Chapter 1: Basic VAE</li> <li>Chapter 2: Extension to Tranformer encoder</li> </ul>"},{"location":"chapter1/","title":"VAE Basics","text":""},{"location":"chapter1/#vae-architecture-1","title":"VAE Architecture 1","text":"<p>Let \\(y_k\\) be measurements of a data generating process. I would like to use the Variational Autoencoder method of statistical inference, to learn about the data generating process. Describe the details of the VAE to accomplish this.</p> <p>Answer</p> <p>Below is a self-contained description of how to set up and train a Variational Autoencoder (VAE) to infer the data-generating process behind measurements \\(y_k\\).  All formulas are in standard LaTeX.</p> <p>1 Model specification    We introduce a low-dimensional latent variable \\(z\\in\\mathbb R^d\\).  The generative model (decoder) and the inference model (encoder) are</p> \\[      \\text{Prior:}\\quad p(z)=\\mathcal N\\bigl(z;0,I\\bigr),      \\quad      \\text{Likelihood:}\\quad p_\\theta(y\\mid z), \\] \\[      \\text{Approx.\\ posterior:}\\quad      q_\\phi(z\\mid y)      =\\mathcal N\\bigl(z;\\,\\mu_\\phi(y),\\,\\mathrm{diag}(\\sigma_\\phi^2(y))\\bigr). \\] <p>Here \\(\\theta\\) are the parameters of the decoder network (which outputs parameters of \\(p_\\theta(y\\mid z)\\)), and \\(\\phi\\) are the parameters of the encoder network (which outputs \\(\\mu_\\phi(y)\\) and \\(\\sigma_\\phi(y)\\)).</p> <p>2 Evidence Lower BOund (ELBO)    For a single datum \\(y\\), the marginal log-likelihood \\(\\log p_\\theta(y)\\) is intractable.  We instead maximize the ELBO</p> \\[      \\mathcal L(\\theta,\\phi\\,;\\,y)      \\;=\\;      \\mathbb{E}_{q_\\phi(z\\mid y)}\\bigl[\\log p_\\theta(y\\mid z)\\bigr]      \\;-\\;      D_{\\mathrm{KL}}\\bigl(q_\\phi(z\\mid y)\\;\\|\\;p(z)\\bigr), \\] <p>which satisfies \\(\\log p_\\theta(y)\\ge\\mathcal L(\\theta,\\phi;y)\\).  Over a dataset \\(\\{y_k\\}_{k=1}^N\\) we maximize</p> \\[      \\mathcal L_{\\rm total}(\\theta,\\phi)      = \\sum_{k=1}^N \\mathcal L(\\theta,\\phi\\,;\\,y_k). \\] <p>3 Reparameterization trick    To backpropagate through the stochastic sampling \\(z\\sim q_\\phi(z\\mid y)\\), we write</p> \\[      z \\;=\\;\\mu_\\phi(y)\\;+\\;\\sigma_\\phi(y)\\odot\\epsilon,      \\quad      \\epsilon\\sim\\mathcal N(0,I). \\] <p>Then</p> \\[      \\mathbb{E}_{q_\\phi(z\\mid y)}\\bigl[\\log p_\\theta(y\\mid z)\\bigr]      \\;=\\;      \\mathbb{E}_{\\epsilon\\sim\\mathcal N(0,I)}      \\bigl[\\log p_\\theta\\bigl(y\\mid \\mu_\\phi(y)+\\sigma_\\phi(y)\\odot\\epsilon\\bigr)\\bigr], \\] <p>which can be unbiasedly estimated with Monte Carlo and differentiated w.r.t.\\ \\(\\phi\\).</p> <p>4 KL-divergence term    For two Gaussians \\(q=\\mathcal N(\\mu,\\mathrm{diag}(\\sigma^2))\\) and \\(p=\\mathcal N(0,I)\\),</p> \\[      D_{\\rm KL}\\bigl(q\\|p\\bigr)      =\\frac12\\sum_{i=1}^d        \\bigl[          \\sigma_i^2+\\mu_i^2-1-\\log(\\sigma_i^2)        \\bigr]. \\] <p>5 Overall training algorithm    Use mini-batches \\(\\{y^{(b)}\\}\\).  For each batch:    a) Encode: compute \\(\\mu_\\phi(y^{(b)})\\), \\(\\sigma_\\phi(y^{(b)})\\).    b) Sample \\(\\epsilon\\sim\\mathcal N(0,I)\\), form \\(z=\\mu_\\phi+\\sigma_\\phi\\odot\\epsilon\\).    c) Decode: compute \\(\\log p_\\theta(y^{(b)}\\mid z)\\).    d) Compute batch ELBO </p> \\[         \\mathcal L_{\\rm batch}          =\\frac1B\\sum_{i=1}^B           \\Bigl[\\log p_\\theta(y^{(i)}\\!\\mid z^{(i)})             -D_{\\rm KL}(q_\\phi(z^{(i)}\\mid y^{(i)})\\|p(z))\\Bigr]. \\] <p>e) Take gradient \\(\\nabla_{\\theta,\\phi}\\mathcal L_{\\rm batch}\\) (e.g.\\ via Adam) and update \\(\\theta,\\phi\\).</p> <p>6 Choice of decoders    \u2013 For real-valued \\(y\\): use Gaussian likelihood \\(\\;p_\\theta(y\\mid z)=\\mathcal N(y;\\,\\mu_\\theta(z),\\,\\mathrm{diag}(\\sigma_y^2))\\).    \u2013 For binary \\(y\\): use Bernoulli \\(\\;p_\\theta(y\\mid z)=\\mathrm{Bernoulli}(y;\\,\\pi_\\theta(z)).\\)</p> <p>7 Inference about the data-generating process    After training, the decoder \\(p_\\theta(y\\mid z)\\) and the aggregated posterior give you a generative model of the data.  You can    \u2022 Sample new synthetic observations \\(y\\sim p_\\theta(y\\mid z)\\) by first drawing \\(z\\sim p(z)\\).    \u2022 Approximate the latent representation of any \\(y\\) by \\(q_\\phi(z\\mid y)\\).    \u2022 Inspect learned features by varying individual components of \\(z\\) and decoding.</p> <p>This completes the detailed VAE setup for learning the underlying data-generating process from measurements \\(y_k\\).</p>"},{"location":"chapter1/#vae-architecture-2-encoder","title":"VAE Architecture 2 - Encoder","text":"<p>Let\u2019s make the setting more precise: the measurements \\(y_k \\in \\mathbb{R}^n\\) and the index \\(k\\). Let \\(Y \\in \\mathbb{R}^n\\) be the random variable (vector) and the \\(y_k\\) are the observations or realizations of it. </p> <p>The encoder takes as input the \\(y_k\\)  and models a probability distribution \\(q_{\\phi}(z | y)\\). Thus, for each \\((k, y_k)\\), the encoder introduces a \\(z_k\\). Is this understanding correct so far?</p> <p>Answer</p> <p>Yes\u2014your understanding is exactly right.  In a VAE we introduce for each observation \\(y_k\\) a corresponding latent variable \\(z_k\\), and the encoder network \u201camortizes\u201d inference by outputting an approximate posterior over \\(z\\) given \\(y\\).  Concretely, one usually chooses</p> \\[ q_{\\phi}(z\\mid y) =\\mathcal N\\bigl(z;\\,\\mu_{\\phi}(y),\\,\\mathrm{diag}(\\sigma_{\\phi}^2(y))\\bigr), \\] <p>so that for each data point \\(y_k\\) you have</p> \\[ q_{\\phi}(z\\mid y_k) =\\mathcal N\\bigl(z;\\,\\mu_{\\phi}(y_k),\\,\\mathrm{diag}(\\sigma_{\\phi}^2(y_k))\\bigr). \\] <p>During training (and inference) you then draw</p> \\[ z_k \\;=\\;\\mu_{\\phi}(y_k)\\;+\\;\\sigma_{\\phi}(y_k)\\odot\\epsilon_k, \\qquad \\epsilon_k\\sim\\mathcal N(0,I), \\] <p>and pass \\(z_k\\) into the decoder \\(p_{\\theta}(y\\mid z)\\).  Thus each \\((k,y_k)\\) is associated with its own latent \\(z_k\\), but the map \\(y\\mapsto(\\mu_{\\phi}(y),\\sigma_{\\phi}(y))\\) is shared across all \\(k\\).</p>"},{"location":"chapter1/#vae-architecture-3-encoder-contd","title":"VAE Architecture 3 - Encoder (contd)","text":"<p>The encoder map \\(y \\mapsto (\\mu_{\\phi}(y), \\sigma_{\\phi}(y))\\) is modeled as a neural network which takes as input the \\(y\\) and outputs \\((\\mu_{\\phi}(y), \\sigma_{\\phi}(y))\\) which are the mean and standard deviation of a Gaussian distribution. So in fact, for each \\(y_k\\), there is a unique Gaussian distribution from which the corresponding \\(z_k\\) is drawn. Is this correct?</p> <p>Answer</p> <p>Yes.  Concretely, the encoder is a deterministic function  </p> \\[ y\\;\\mapsto\\;\\bigl(\\mu_\\phi(y),\\,\\sigma_\\phi(y)\\bigr)\\in\\mathbb R^d\\times\\mathbb R^d_{&gt;0}, \\] <p>so that for each observation \\(y_k\\) you obtain parameters of a Gaussian,  </p> \\[ q_\\phi(z\\mid y_k) =\\mathcal N\\bigl(z;\\,\\mu_\\phi(y_k),\\,\\mathrm{diag}(\\sigma_\\phi(y_k)^2)\\bigr). \\] <p>You then draw  </p> \\[ z_k \\;=\\;\\mu_\\phi(y_k)\\;+\\;\\sigma_\\phi(y_k)\\odot\\epsilon_k, \\quad \\epsilon_k\\sim\\mathcal N(0,I), \\] <p>so indeed each \\(y_k\\) induces its own Gaussian from which \\(z_k\\) is sampled.</p>"},{"location":"chapter1/#sampling-the-latent-variable-z-reparameterization-trick","title":"Sampling the latent variable \\(z\\): Reparameterization trick","text":"<p>It would seem that sampling \\(z_k\\) from the Gaussian directly, that is $ z_k \\sim \\mathcal{N}(z; \\mu_{\\phi}(y_k), diag(\\sigma_{\\phi}(y_k)^2))$ is more natural than in the manner you describe.</p> <p>Answer</p> <p>Your intuition is absolutely right: in the VAE we do want to \u201creally\u201d draw  </p> \\[ z_k \\;\\sim\\;\\mathcal N\\!\\bigl(\\mu_{\\phi}(y_k)\\,,\\,\\mathrm{diag}(\\sigma_{\\phi}(y_k)^2)\\bigr) \\] <p>for each data\u2010point \\(y_k\\).  The reason we write it in the form</p> \\[ z_k \\;=\\;\\mu_{\\phi}(y_k)\\;+\\;\\sigma_{\\phi}(y_k)\\odot\\epsilon_k, \\quad \\epsilon_k\\sim\\mathcal N(0,I) \\] <p>is simply a reparameterization of that same Gaussian draw.  Algebraically the two are equivalent.  The reparameterization is introduced only so that when you back\u2010propagate the reconstruction\u2010loss </p> \\[ \\mathbb{E}_{q_\\phi(z\\mid y_k)}\\bigl[\\log p_\\theta(y_k\\mid z)\\bigr] \\] <p>through the encoder network, you get low\u2010variance, unbiased gradients w.r.t.\\ \\(\\phi\\).  </p> <p>If you were to write  </p> \\[ z_k \\;\\sim\\;\\mathcal N\\bigl(\\mu_{\\phi}(y_k),\\,\\mathrm{diag}(\\sigma_{\\phi}(y_k)^2)\\bigr) \\] <p>and then try to differentiate directly through that sampling operation, the path from \\(\\phi\\) into the sample \\(z_k\\) would be discontinuous and you would have to resort to score\u2010function estimators (REINFORCE), which typically have much higher variance.</p> <p>Thus: 1  Yes, each \\(y_k\\) defines its own Gaussian for \\(z_k\\). 2  Writing the draw via  </p> \\[      z_k = \\mu_\\phi(y_k) + \\sigma_\\phi(y_k)\\odot\\epsilon_k \\] <p>is just a mathematically equivalent way of expressing  </p> \\[      z_k\\sim\\mathcal N\\bigl(\\mu_\\phi(y_k),\\,\\mathrm{diag}(\\sigma_\\phi(y_k)^2)\\bigr) \\] <p>that makes it possible to back\u2010propagate gradients through the random draw.</p>"},{"location":"chapter1/#reparameterization-trick-2","title":"Reparameterization Trick 2","text":"<p>Show the mathematical equivalence of the two ways of drawing \\(z_k\\) rigorously.</p> <p>Answer</p> <p>Below is a standard \u201cchange\u2010of\u2010variables\u201d proof (using the Jacobian) and an alternative proof via moment\u2010generating functions, which together show rigorously that</p> \\[ z=\\mu+\\sigma\\odot\\epsilon,\\quad\\epsilon\\sim\\mathcal N(0,I_d) \\] <p>has exactly the same law as</p> \\[ z\\sim\\mathcal N\\bigl(\\mu,\\;\\mathrm{diag}(\\sigma^2)\\bigr). \\] <p>1  Change\u2010of\u2010variables (Jacobian) proof    Let \\(d\\) be the dimension of \\(z\\).  Write \\(\\sigma=(\\sigma_1,\\dots,\\sigma_d)\\) and define the diagonal matrix</p> <p>$$      \\Sigma=\\mathrm{diag}(\\sigma_1,\\dots,\\sigma_d)\\in\\mathbb R^{d\\times d}.    $$</p> <p>We draw</p> <p>$$      \\epsilon\\sim p_\\epsilon(\\epsilon)      =\\frac1{(2\\pi)^{d/2}}        \\exp!\\Bigl(-\\tfrac12\\,\\epsilon^\\top\\epsilon\\Bigr),      \\qquad      z \\;=\\;\\mu+\\Sigma\\,\\epsilon.    $$</p> <p>Since the map</p> <p>$$      T:\\;\\epsilon\\mapsto z=T(\\epsilon)=\\mu+\\Sigma\\,\\epsilon    $$</p> <p>is invertible with</p> <p>$$      T^{-1}(z)=\\Sigma^{-1}(z-\\mu),      \\quad      \\det\\Bigl(\\tfrac{\\partial T^{-1}}{\\partial z}\\Bigr)      =\\det(\\Sigma^{-1})      =\\prod_{i=1}^d\\frac1{\\sigma_i}\\,,    $$</p> <p>the density of \\(z\\) is</p> \\[      p_z(z)      =p_\\epsilon\\bigl(T^{-1}(z)\\bigr)       \\;\\Bigl|\\det\\bigl(\\tfrac{\\partial T^{-1}}{\\partial z}\\bigr)\\Bigr|      =\\frac1{(2\\pi)^{d/2}}       \\exp\\!\\Bigl(-\\tfrac12\\|\\Sigma^{-1}(z-\\mu)\\|^2\\Bigr)       \\;\\prod_{i=1}^d\\frac1{\\sigma_i}. \\] <p>Noting     \\(\\|\\Sigma^{-1}(z-\\mu)\\|^2=\\sum_i\\frac{(z_i-\\mu_i)^2}{\\sigma_i^2}\\)    and \\(\\prod_i\\sigma_i^{-1}=(\\det\\Sigma)^{-1}\\), this becomes</p> <p>$$      p_z(z)      =\\frac1{(2\\pi)^{d/2}\\,\\prod_i\\sigma_i}       \\exp!\\Bigl(-\\tfrac12\\sum_{i=1}^d\\frac{(z_i-\\mu_i)^2}{\\sigma_i^2}\\Bigr),    $$</p> <p>which is exactly the density of \\(\\mathcal N(\\mu,\\mathrm{diag}(\\sigma^2))\\).</p> <p>2  Moment\u2010Generating\u2010Function (MGF) proof    Recall the MGF of a random vector \\(w\\) is \\(M_w(t)=\\mathbb E[e^{t^\\top w}]\\).  If \\(w\\sim\\mathcal N(0,I)\\), then \\(M_w(t)=\\exp(\\tfrac12\\,t^\\top t)\\).  Now set</p> <p>$$      z=\\mu+\\Sigma\\,\\epsilon,      \\quad      \\epsilon\\sim\\mathcal N(0,I).    $$</p> <p>Then for any \\(t\\in\\mathbb R^d\\),</p> \\[      M_z(t)      =\\mathbb E\\bigl[e^{t^\\top z}\\bigr]      =\\mathbb E\\bigl[e^{\\,t^\\top(\\mu+\\Sigma\\epsilon)}\\bigr]      =e^{\\,t^\\top\\mu}\\;\\mathbb E\\bigl[e^{\\,(t^\\top\\Sigma)\\epsilon}\\bigr]      =e^{\\,t^\\top\\mu}\\;\\exp\\!\\Bigl(\\tfrac12\\,(t^\\top\\Sigma)(t^\\top\\Sigma)^\\top\\Bigr). \\] <p>But \\((t^\\top\\Sigma)(t^\\top\\Sigma)^\\top=t^\\top\\,\\Sigma^2\\,t\\) and \\(\\Sigma^2=\\mathrm{diag}(\\sigma^2)\\).  Hence</p> <p>$$      M_z(t)      =\\exp!\\Bigl(t^\\top\\mu+\\tfrac12\\,t^\\top\\mathrm{diag}(\\sigma^2)\\,t\\Bigr),    $$</p> <p>which is precisely the MGF of \\(\\mathcal N(\\mu,\\mathrm{diag}(\\sigma^2))\\).  By uniqueness of MGFs,  </p> <p>$$      z\\;\\sim\\;\\mathcal N\\bigl(\\mu,\\mathrm{diag}(\\sigma^2)\\bigr).    $$</p> <p>Conclusion: drawing </p> \\[ z_k\\;\\sim\\;\\mathcal N\\bigl(\\mu_\\phi(y_k),\\,\\mathrm{diag}(\\sigma_\\phi(y_k)^2)\\bigr) \\] <p>at the sampling step is mathematically equivalent to writing </p> \\[ z_k=\\mu_\\phi(y_k)+\\sigma_\\phi(y_k)\\odot\\epsilon_k, \\quad \\epsilon_k\\sim\\mathcal N(0,I), \\] <p>since both constructions produce the same Gaussian law for \\(z_k\\).</p>"},{"location":"chapter1/#vae-architecture-4-decoder","title":"VAE Architecture 4 - Decoder","text":"<p>For the decoder, it appears we have a neural network which models  \\(p_\\theta(y\\mid z)=\\mathcal N(y;\\,\\mu_\\theta(z),\\,\\mathrm{diag}(\\sigma_{\\theta}^2(z)))\\). The input to this neural network is \\(z_k\\) and the output are the Gaussian parameters \\(\\mu_{\\theta}(z_k)\\) and \\(\\sigma_{\\theta}(z_k)\\). The reconstruction \\(\\tilde{y}_k\\) is drawn from this Gaussian distribution.</p> <p>Answer</p> <p>Yes, that\u2019s exactly the usual Gaussian\u2010decoder VAE.  Concretely:</p> <p>1 Decoder network    You have a deterministic neural network  </p> \\[      z\\;\\longmapsto\\;\\bigl(\\mu_\\theta(z),\\,\\sigma_\\theta(z)\\bigr)      \\;\\in\\;\\mathbb R^n\\times\\mathbb R^n_{&gt;0}, \\] <p>parameterized by \\(\\theta\\).  Here \\(\\mu_\\theta(z)\\) and \\(\\sigma_\\theta(z)\\) are the per-coordinate means and standard deviations of your Gaussian likelihood.</p> <p>2 Likelihood    Given a latent \\(z_k\\), you define</p> \\[      p_\\theta\\bigl(y\\mid z_k\\bigr)      =\\mathcal N\\!\\bigl(y\\;;\\;\\mu_\\theta(z_k),\\;\\mathrm{diag}(\\sigma_\\theta(z_k)^2)\\bigr). \\] <p>3 Reconstruction sample    To generate (or \u201creconstruct\u201d) a sample \\(\\tilde y_k\\) from the decoder you draw</p> <p>$$      \\tilde y_k      \\;=\\;\\mu_\\theta(z_k)\\;+\\;\\sigma_\\theta(z_k)\\odot\\epsilon,      \\qquad      \\epsilon\\sim\\mathcal N(0,I_n).    $$</p> <p>Equivalently,</p> <p>$$      \\tilde y_k\\;\\sim\\;\\mathcal N!\\bigl(\\mu_\\theta(z_k),\\,\\mathrm{diag}(\\sigma_\\theta(z_k)^2)\\bigr).    $$</p> <p>4 ELBO term    In practice, when computing the reconstruction term of the ELBO you don\u2019t need to draw \\(\\tilde y_k\\) explicitly.  You compute</p> \\[      \\mathbb E_{q_\\phi(z\\mid y_k)}\\Bigl[\\log p_\\theta(y_k\\mid z)\\Bigr]      \\;\\approx\\;      \\frac1L\\sum_{\\ell=1}^{L}      \\log \\mathcal N\\!\\bigl(y_k\\;;\\;\\mu_\\theta(z_k^{(\\ell)}),\\,\\mathrm{diag}(\\sigma_\\theta(z_k^{(\\ell)})^2)\\bigr) \\] <p>using samples \\(z_k^{(\\ell)}\\) from \\(q_\\phi(z\\mid y_k)\\).  For a single sample the Gaussian log-density is</p> \\[      \\log p_\\theta(y_k\\mid z_k)      =-\\frac12\\sum_{i=1}^n\\Bigl[        \\tfrac{\\bigl(y_{k,i}-\\mu_{\\theta,i}(z_k)\\bigr)^2}{\\sigma_{\\theta,i}(z_k)^2}        +\\log\\bigl(2\\pi\\,\\sigma_{\\theta,i}(z_k)^2\\bigr)      \\Bigr]. \\] <p>So yes: the decoder takes \\(z_k\\) as input, outputs \\(\\mu_\\theta(z_k)\\) and \\(\\sigma_\\theta(z_k)\\), and then the reconstruction \\(\\tilde y_k\\) is sampled from that Gaussian.</p>"},{"location":"chapter1/#training","title":"Training","text":"<p>Now that we have clarified the architecture of the VAE, let\u2019s discuss the details of the training. The observation index \\(k = 1, \\cdots, m\\). There are \\(m\\) observations. \\(y_k \\in \\mathbb{R}^n\\).  What is the objective function for training? Clearly there must be a term to minimize the reconstruction error \\((y_k - \\tilde{y}_k)\\). The training phase must result in fixing of the parameters \\(\\phi\\) and \\(\\theta\\) of the encoder and decoder networks. Explain the details of training.</p> <p>Answer</p> <p>The training objective of a VAE is to maximize, over your encoder\u2010parameters \\(\\phi\\) and decoder\u2010parameters \\(\\theta\\), the (empirical) Evidence Lower BOund (ELBO) on the log\u2010likelihood of your \\(m\\) data points \\(\\{y_k\\}_{k=1}^m\\).  Equivalently one minimizes the negative ELBO, which decomposes into a reconstruction term plus a regularizing KL term.  Concretely:</p> <ol> <li> <p>Definition of the ELBO for a single datum     For each observation \\(y_k\\), the ELBO is</p> \\[   \\mathcal L(\\theta,\\phi;\\,y_k)   \\;=\\;   \\underbrace{\\mathbb{E}_{z\\sim q_\\phi(z\\mid y_k)}        \\bigl[\\log p_\\theta(y_k\\mid z)\\bigr]}_{\\displaystyle\\text{(a) reconstruction}}    \\;-\\;   \\underbrace{D_{\\rm KL}\\bigl(q_\\phi(z\\mid y_k)\\,\\|\\,p(z)\\bigr)}_     {\\displaystyle\\text{(b) regularizer}}. \\] <p>Here (a) \\(\\mathbb{E}_{q_\\phi}[\\log p_\\theta(y_k\\mid z)]\\) encourages the decoder to place high likelihood on \\(y_k\\), (b) \\(D_{\\rm KL}(q_\\phi(z\\mid y_k)\\,\\|\\,p(z))\\) pushes the approximate posterior toward the prior \\(p(z)=\\mathbb{N}(0,I)\\).</p> </li> <li> <p>Empirical objective over the dataset     Replace the population expectation by the empirical average over \\(k=1,\\dots,m\\).  The total ELBO is</p> \\[   \\mathcal L_{\\rm total}(\\theta,\\phi)   = \\sum_{k=1}^m \\mathcal L(\\theta,\\phi;\\,y_k)   \\;=\\;   \\sum_{k=1}^m   \\Bigl\\{   \\mathcal{E}_{q_\\phi(z\\mid y_k)}\\bigl[\\log p_\\theta(y_k\\mid z)\\bigr]   \\;-\\;   D_{\\rm KL}\\bigl(q_\\phi(z\\mid y_k)\\,\\|\\,p(z)\\bigr)   \\Bigr\\}. \\] <p>In practice we maximize \\(\\mathcal L_{\\rm total}\\), or equivalently minimize the loss</p> \\[   \\mathcal J(\\theta,\\phi)   = -\\,\\mathcal L_{\\rm total}(\\theta,\\phi). \\] </li> <li> <p>Writing out the two terms     3.1 Reconstruction term     If you use a Gaussian decoder  </p> \\[   p_\\theta(y\\mid z)   = \\mathbb{N}\\!\\bigl(y;\\,\\mu_\\theta(z),\\,\\text{diag}(\\sigma_\\theta(z)^2)\\bigr), \\] <p>then</p> \\[   \\log p_\\theta(y_k\\mid z)   = -\\frac12\\sum_{i=1}^n     \\Bigl[\\tfrac{(y_{k,i}-\\mu_{\\theta,i}(z))^2}{\\sigma_{\\theta,i}(z)^2}           +\\log\\bigl(2\\pi\\,\\sigma_{\\theta,i}(z)^2\\bigr)     \\Bigr]. \\] <p>In many implementations one further fixes \\(\\sigma_{\\theta,i}(z)=1\\), so that \\(\\log p_\\theta(y_k\\mid z)\\propto -\\tfrac12\\|y_k-\\mu_\\theta(z)\\|^2\\), i.e.\\ a mean\u2010squared error.</p> <p>3.2 KL\u2010divergence term Since</p> \\[   q_\\phi(z\\mid y_k)   = \\mathbb{N}\\!\\bigl(z;\\,\\mu_\\phi(y_k),\\,\\text{diag}(\\sigma_\\phi(y_k)^2)\\bigr),   \\quad   p(z)=\\mathbb{N}(0,I), \\] <p>one has the closed\u2010form</p> \\[   D_{\\rm KL}\\bigl(q_\\phi(z\\mid y_k)\\,\\|\\,p(z)\\bigr)   = \\frac12\\sum_{j=1}^d     \\bigl[\\sigma_{\\phi,j}(y_k)^2           +\\mu_{\\phi,j}(y_k)^2           -1           -\\log\\sigma_{\\phi,j}(y_k)^2     \\bigr]. \\] </li> <li> <p>Stochastic optimization via mini\u2010batches     You typically do not compute the full sum over \\(k=1,\\dots,m\\) at each step.  Instead you sample a mini\u2010batch \\(\\{y_{k_1},\\dots,y_{k_B}\\}\\) of size \\(B\\), and approximate the ELBO by</p> \\[   \\widehat{\\mathcal L}_{\\rm batch}   = \\frac1B\\sum_{b=1}^B   \\Bigl\\{     \\log p_\\theta\\bigl(y_{k_b}\\mid z_{k_b}\\bigr)     \\;-\\;     D_{\\rm KL}\\bigl(q_\\phi(z\\mid y_{k_b})\\|\\;p(z)\\bigr)   \\Bigr\\}, \\] <p>where each \\(z_{k_b}\\) is drawn via the reparameterization</p> \\[   z_{k_b}   = \\mu_\\phi\\bigl(y_{k_b}\\bigr) + \\sigma_\\phi\\bigl(y_{k_b}\\bigr)\\odot \\epsilon,\\quad   \\epsilon\\sim\\mathbb{N}(0,I_d). \\] <p>You then take one (or several) gradient steps</p> \\[   \\phi\\;\\leftarrow\\;\\phi + \\alpha\\,\\nabla_\\phi\\,\\widehat{\\mathcal L}_{\\rm batch},   \\qquad   \\theta\\;\\leftarrow\\;\\theta + \\alpha\\,\\nabla_\\theta\\,\\widehat{\\mathcal L}_{\\rm batch}, \\] <p>using, e.g., the Adam optimizer with learning rate \\(\\alpha\\).</p> </li> </ol> <p>5   Summary of the training loop</p> <p>For \\(t=1,\\dots,T\\) (number of gradient steps):</p> <p>a) Sample a mini\u2010batch \\(\\{y_{k_b}\\}_{b=1}^B\\).  b) For each \\(y_{k_b}\\) compute \\(\\mu_\\phi(y_{k_b})\\), \\(\\sigma_\\phi(y_{k_b})\\).  c) Sample \\(\\epsilon_b\\sim\\mathbb{N}(0,I_d)\\), set  </p> \\[             z_{k_b}=\\mu_\\phi(y_{k_b})+\\sigma_\\phi(y_{k_b}) \\odot\\epsilon_b. \\] <p>d) Decode: compute \\(\\mu_\\theta(z_{k_b})\\), \\(\\sigma_\\theta(z_{k_b})\\) and then \\(\\log p_\\theta(y_{k_b}\\mid z_{k_b})\\).  e) Compute the batch ELBO \\(\\widehat{\\mathcal L}_{\\rm batch}\\) and its gradients w.r.t.\\ \\(\\phi,\\theta\\).  f) Update \\(\\phi,\\theta\\) by a gradient\u2010based optimizer.</p> <p>After convergence, you have fixed the encoder parameters \\(\\phi\\) and decoder parameters \\(\\theta\\).  At test time you can (i) encode any new \\(y\\) to \\(q_\\phi(z\\mid y)\\), (ii) sample \\(z\\sim p(z)\\) and decode to generate new data via \\(p_\\theta(y\\mid z)\\).</p>"},{"location":"chapter1/#training-2","title":"Training 2","text":"<p>In the summary above, you introduced \\(t = 1, \\cdots, T\\). What is this?</p> <p>Answer</p> <p>Here, \\(t\\) simply indexes the training iterations (or \u201csteps\u201d) of your stochastic optimization loop.  Concretely:</p> <ul> <li>You run a loop of \\(T\\) total gradient\u2010update steps.  </li> <li>At each step \\(t=1,2,\\dots,T\\), you  </li> <li>sample a mini-batch of data,  </li> <li>compute the stochastic estimate of the ELBO,  </li> <li>compute its gradients w.r.t.\\ \\(\\phi,\\theta\\), and  </li> <li>take a gradient step to update \\(\\phi,\\theta\\).  </li> </ul> <p>Thus  </p> \\[ t=1,\\dots,T \\] <p>labels each of these updates, and \\(T\\) is the total number of such mini-batch update steps you choose to perform.</p>"},{"location":"chapter1/#reconstruction-term-in-elbo","title":"Reconstruction term in ELBO","text":"<p>I need clarification on the reconstruction term in ELBO. Reconstruction should mean that given \\(y_k\\), the VAE generates an estimate \\(\\tilde{y}_k\\) such that the difference is small in some measure. How does this idea translate to the reconstruction term used in ELBO. Explain in a step by step manner.</p> <p>Answer</p> <p>Below is a step-by-step account of how the intuitive notion of \u201creconstruction error\u201d  </p> \\[ \\|y_k - \\tilde y_k\\| \\] <p>is embedded into the ELBO\u2019s reconstruction term \\(\\displaystyle \\mathbb{E}_{q_\\phi(z\\mid y_k)}[\\log p_\\theta(y_k\\mid z)]\\).</p> <p>1  Choose a probabilistic decoder \\(p_\\theta(y\\mid z)\\)     Rather than producing a single point estimate \\(\\tilde y_k\\), the decoder outputs a probability density over possible reconstructions.  A common choice for real-valued data is  </p> <p>$$       p_\\theta(y\\mid z)       =\\mathcal{N}!\\bigl(y;\\,\\mu_\\theta(z),\\,\\sigma^2 I\\bigr),   $$</p> <p>where \\(\\mu_\\theta(z)\\in\\mathbb{R}^n\\) is the decoder\u2019s mean and \\(\\sigma^2\\) is a (fixed or learned) variance.</p> <p>2  Log-likelihood as \u201cnegative reconstruction error\u201d     For a Gaussian likelihood the log-density of observing \\(y_k\\) given \\(z\\) is</p> <p>$$       \\log p_\\theta(y_k\\mid z)       = -\\,\\frac{1}{2\\sigma^2}\\,\\bigl|y_k - \\mu_\\theta(z)\\bigr|^2         \\;-\\;\\frac{n}{2}\\log(2\\pi\\sigma^2).   $$     Up to the additive constant \\(-\\tfrac{n}{2}\\log(2\\pi\\sigma^2)\\), this is exactly \\(-\\tfrac1{2\\sigma^2}\\) times the squared error between \\(y_k\\) and the reconstruction mean \\(\\mu_\\theta(z)\\).</p> <p>3  Incorporating the encoder uncertainty     The VAE does not pick a single \\(z\\) but rather a distribution \\(q_\\phi(z\\mid y_k)\\).  Hence we take the expectation of the log-likelihood under \\(z\\sim q_\\phi(z\\mid y_k)\\):</p> \\[   \\mathbb{E}_{z\\sim q_\\phi(z\\mid y_k)}\\!\\bigl[\\log p_\\theta(y_k\\mid z)\\bigr]  =    -\\,\\frac{1}{2\\sigma^2}\\,   \\mathbb{E}_{q_\\phi}\\!\\bigl[\\|y_k-\\mu_\\theta(z)\\|^2\\bigr]   \\;+\\;\\text{constant}. \\] <p>Maximizing this term forces the decoder\u2019s mean \\(\\mu_\\theta(z)\\) (averaged over plausible \\(z\\) values) to lie close to the true \\(y_k\\).</p> <p>4  Monte-Carlo approximation     In practice we approximate the expectation by drawing one (or a few) samples </p> <p>$$       z_k = \\mu_\\phi(y_k)+\\sigma_\\phi(y_k)\\odot\\epsilon,       \\quad       \\epsilon\\sim\\mathcal{N}(0,I),   $$</p> <p>and computing</p> <p>$$       \\log p_\\theta(y_k\\mid z_k)       \\;\\approx\\;       -\\frac{1}{2\\sigma^2}\\,\\bigl|y_k-\\mu_\\theta(z_k)\\bigr|^2       \\;+\\;\\text{const}.   $$</p> <p>5  ELBO reconstruction term     Putting it all together, the reconstruction part of the ELBO for datum \\(k\\) is</p> \\[       \\mathbb{E}_{q_\\phi(z\\mid y_k)}\\bigl[\\log p_\\theta(y_k\\mid z)\\bigr]       \\;\\approx\\;       -\\,\\frac{1}{2\\sigma^2}\\,       \\|y_k - \\mu_\\theta(z_k)\\|^2       \\;+\\;\\text{const}. \\] <p>Thus maximizing this expectation is equivalent (up to constants) to minimizing the expected squared error     \\(\\|y_k-\\mu_\\theta(z)\\|^2\\)\u2014i.e.\\ the usual reconstruction loss.</p> <p>6  Summary   - The decoder\u2019s density \\(p_\\theta(y\\mid z)\\) turns point-wise reconstruction into a log-likelihood.   - For Gaussian \\(p_\\theta\\), \\(\\log p_\\theta(y_k\\mid z)\\) is (negative) MSE plus a constant.   - Taking \\(\\mathbb{E}_{q_\\phi(z\\mid y_k)}\\) yields an average reconstruction error weighted by how likely each \\(z\\) is under the encoder.   - Maximizing this term in the ELBO drives \\(\\mu_\\theta(z)\\) to match \\(y_k\\), exactly implementing the intuitive \u201creconstruction error\u201d objective.</p>"},{"location":"chapter1/#regularization-term-in-elbo","title":"Regularization term in ELBO","text":"<p>Explain the intuition behind the regularization term in ELBO.</p> <p>Answer</p> <p>The ELBO for one data point \\(y_k\\) can be written as  </p> \\[ \\mathcal{L}(\\theta,\\phi; y_k) = \\underbrace{\\mathbb{E}_{q_\\phi(z\\mid y_k)}\\bigl[\\log p_\\theta(y_k\\mid z)\\bigr]}_{\\displaystyle\\text{(reconstruction)}}  \\;-\\; \\underbrace{D_{\\mathrm{KL}}\\bigl(q_\\phi(z\\mid y_k)\\,\\big\\|\\,p(z)\\bigr)}_{\\displaystyle\\text{(regularizer)}}. \\] <p>You already saw that the first term rewards accurate reconstructions.  We now build intuition for the second term  </p> \\[ D_{\\mathrm{KL}}\\bigl(q_\\phi(z\\mid y_k)\\,\\big\\|\\,p(z)\\bigr) =\\int q_\\phi(z\\mid y_k)\\,\\log\\frac{q_\\phi(z\\mid y_k)}{p(z)}\\,dz. \\] <ol> <li> <p>Preventing \u201cover\u2010powerful\u201d encodings    If we only maximized reconstruction, the encoder could drive \\(q_\\phi(z\\mid y_k)\\) to a Dirac delta at some \\(z\\) that decodes perfectly to \\(y_k\\).  That would give zero reconstruction error but completely \u201coverfit\u201d each \\(y_k\\) (and destroy any ability to sample new \\(z\\sim p(z)\\)).  The KL term penalizes deviation of \\(q_\\phi\\) from the prior \\(p(z)\\), blocking this collapse.</p> </li> <li> <p>Encouraging a \u201csmooth,\u201d compressive code    The KL measures how many \u201cnats\u201d (or bits) of information the approximate posterior carries beyond the prior.      \u2022 If \\(q_\\phi(z\\mid y_k)\\approx p(z)\\), then \\(D_{\\mathrm{KL}}\\approx0\\): the code for \\(y_k\\) is very compact (almost no information over the prior).      \u2022 If \\(q_\\phi(z\\mid y_k)\\) sharpens far away from \\(p(z)\\), the KL grows, penalizing your objective.    Thus you trade off fidelity (reconstruction) versus compression (staying close to \\(p(z)\\)).</p> </li> <li> <p>Making generation from \\(p(z)\\) meaningful    After training, you want to sample \\(z\\sim p(z)\\) and decode via \\(p_\\theta(y\\mid z)\\).  If during training the posterior encodings \\(q_\\phi(z\\mid y)\\) wandered far from \\(p(z)\\), then samples from \\(p(z)\\) would lie in regions the decoder never saw\u2014leading to garbage outputs.  The KL term aligns the bulk of \\(q_\\phi(z\\mid y)\\) with \\(p(z)\\), so that \u201crandom\u201d \\(z\\sim p(z)\\) produce sensible \\(y\\).</p> </li> <li> <p>Interpreting as a regularizer    In machine\u2010learning terms, \\(D_{\\mathrm{KL}}(q\\|p)\\) is a complexity penalty on the encoder.  If you view \\(q_\\phi(z\\mid y_k)\\) as a variational approximation with free parameters \\(\\phi\\), the KL term is the analog of an \\(\\ell_2\\)-penalty on those parameters\u2014inhibiting them from drifting arbitrarily far from a reference distribution \\(p(z)\\).</p> </li> <li> <p>Rate\u2013Distortion perspective    The VAE objective exactly mirrors the classical rate\u2013distortion trade\u2010off:      \u2013 Distortion \\(\\equiv\\) expected reconstruction loss \\(\\mathbb{E}[-\\log p_\\theta(y\\mid z)]\\).      \u2013 Rate \\(\\equiv\\) KL divergence \\(D_{\\mathrm{KL}}(q_\\phi(z\\mid y)\\|p(z))\\).    By tuning this trade\u2010off, you decide how much information about \\(y\\) the latent code \\(z\\) is allowed to carry.</p> </li> </ol> <p>In practice, you cannot drive the KL term to zero without wrecking reconstruction, nor can you drive it to huge values without overfitting.  The optimum balances the two, yielding latent representations that are both informative about each \\(y_k\\) and yet remain well\u2010aligned with the prior \\(p(z)\\).</p>"},{"location":"chapter1/#inference-pass","title":"Inference pass","text":"<p>I will come back to the intuition for the KL term. During the inference forward pass phase, how does the VAE work? Certainly, new observations \\(y\\) are the input. Then what?</p> <p>Answer</p> <p>At inference time (i.e. after you have finished training and fixed \\(\\phi,\\theta\\)), the VAE acts as a fast \u201cencoder\u2192latent\u2192optional decoder\u201d pipeline.  Concretely, given a new observation \\(y\\in\\Bbb R^n\\):</p> <p>1 Encode \\(y\\) to an approximate posterior</p> <p>You pass \\(y\\) through the encoder network and compute  </p> \\[      (\\mu,\\sigma)      \\;=\\;      \\bigl(\\mu_{\\phi}(y),\\,\\sigma_{\\phi}(y)\\bigr),      \\quad      q_{\\phi}(z\\mid y)      =\\mathcal{N}\\!\\bigl(z;\\,\\mu,\\;\\mathrm{diag}(\\sigma^2)\\bigr). \\] <p>2 Obtain a latent code \\(z\\), You have two main choices:</p> <p>(a) Deterministic embedding: use the posterior mean  </p> \\[          z = \\mu_{\\phi}(y). \\] <p>This gives a single \u201cbest\u2010guess\u201d embedding of \\(y\\). (b) Stochastic sample: draw  </p> \\[          z = \\mu_{\\phi}(y)\\;+\\;\\sigma_{\\phi}(y)\\odot\\epsilon,          \\quad          \\epsilon\\sim\\mathcal{N}(0,I_d). \\] <p>This reflects the encoder\u2019s uncertainty in the latent representation.</p> <p>3 (Optional) Reconstruct \\(y\\)    If you wish to see how well the model \u201creconstructs\u201d \\(y\\), you pass \\(z\\) through the decoder network:  </p> \\[      (\\tilde\\mu,\\tilde\\sigma)      = \\bigl(\\mu_{\\theta}(z),\\,\\sigma_{\\theta}(z)\\bigr),      \\quad      p_{\\theta}(y\\mid z)      =\\mathcal{N}(y;\\,\\tilde\\mu,\\text{diag}(\\tilde\\sigma^2)). \\] <p>Again you may either take \\(\\tilde y = \\tilde\\mu\\) as the point reconstruction, or sample \\(\\tilde y \\sim \\mathcal{N}(\\tilde\\mu,\\text{diag}(\\tilde\\sigma^2))\\).</p> <p>4 Use of the latent code    Once you have \\(z\\), you can    \u2022 Feed it into downstream tasks (classification, clustering, regression),    \u2022 Interpolate in latent space between different \\(z\\)s and decode back to data\u2010space,    \u2022 Visualize or otherwise analyze the low\u2010dimensional manifold captured by the encoder.</p> <p>Because the encoder and decoder are just feed\u2010forward neural networks at this point, each new \\(y\\) is processed in a single forward pass through the encoder (and optionally through the decoder), with no further gradient updates.</p>"},{"location":"chapter1/#joint-distribution-of-y","title":"Joint Distribution of \\(y\\)","text":"<p>Since \\(y \\in \\mathbb{R}^n\\) represents a realization of the data generating process, each entry of \\(y\\), that is, \\(y^{(i)}, i = 1, \\cdots, n\\) can be viewed as a random variable. After the VAE has been trained, it can be used to model the joint distribution of the \\(y^{(i)}\\). Is this correct?</p> <p>Answer</p> <p>Yes \u2014 once the VAE is trained, you have in hand a full probabilistic model of the random vector </p> \\[ Y=(Y^{(1)},\\dots,Y^{(n)})\\in\\mathbb{R}^n \\] <p>via the decoder \\(p_\\theta(y\\mid z)\\) together with the prior \\(p(z)\\).  Concretely you define the model\u2019s marginal (joint) density on \\(Y\\) by  </p> \\[ p_\\theta(y) =\\int p(z)\\,p_\\theta(y\\mid z)\\,\\mathrm{d}z \\;=\\; \\int \\mathcal N\\bigl(z;0,I_d\\bigr)\\,        \\mathcal N\\bigl(y;\\mu_\\theta(z),\\text{diag}(\\sigma_\\theta(z)^2)\\bigr)        \\,\\mathrm{d}z. \\] <p>This \\(p_\\theta(y)\\) is your learned approximation to the true data\u2010generating joint density of \\(\\{Y^{(i)}\\}_{i=1}^n\\).  </p> <p>1  Conditional independence structure   Note that in the Gaussian\u2010decoder VAE we assume   $$       p_\\theta(y\\mid z)       =\\prod_{i=1}^n         \\mathcal N\\bigl(y^{(i)};\\,\\mu_{\\theta,i}(z),\\,\\sigma_{\\theta,i}(z)^2\\bigr).   $$   Hence the coordinates \\(Y^{(1)},\\dots,Y^{(n)}\\) are conditionally independent given \\(Z\\), but they are marginally dependent because they all couple through the latent \\(Z\\).</p> <p>2  Sampling from the joint   To generate a joint sample \\(\\tilde y\\sim p_\\theta(y)\\):     a) draw \\(z\\sim p(z)=\\mathcal N(0,I_d)\\),     b) draw \\(\\tilde y\\sim p_\\theta(y\\mid z)\\).   You thus obtain one sample of the full vector \\((\\tilde y^{(1)},\\dots,\\tilde y^{(n)})\\).</p> <p>3  Density evaluation   Although the integral \\(\\;p_\\theta(y)=\\int p(z)\\,p_\\theta(y\\mid z)\\,dz\\)   is not available in closed form, you can approximate it (for instance via importance sampling or other Monte Carlo methods) if you need to evaluate the joint density of a given \\(y\\).</p> <p>4  Approximation caveats   \u2013 The true data\u2010generating joint density \\(p_{\\rm data}(y)\\) may lie outside your model family \\(\\{p_\\theta\\}\\).  Thus \\(p_\\theta(y)\\) is in general only an approximation, the quality of which depends on network capacity and optimization.   \u2013 Because we optimized an ELBO (a lower bound on \\(\\log p_\\theta(y)\\)), the learned \\(\\theta\\) maximizes that bound, not the exact likelihood.  Still, in practice this often gives a very good fit to the joint.</p> <p>In summary, after training your VAE defines a generative model </p> \\[ Y\\;\\sim\\;p_\\theta(y) \\;=\\; \\int p(z)\\,p_\\theta(y\\mid z)\\,dz, \\] <p>which is a model of the joint distribution of the entries \\(Y^{(1)},\\dots,Y^{(n)}\\).</p>"},{"location":"chapter1/#elbo-derived","title":"ELBO derived","text":"<p>You have provided the intuition for ELBO. Now provide a formal derivation of ELBO.</p> <p>Answer</p> <p>Below is the standard \u201cvariational\u201d derivation of the ELBO (Evidence Lower BOund) for a single datum \\(y\\).  All steps use only basic properties of logarithms, expectations, and Jensen\u2019s inequality.</p> <p>1  The marginal log\u2010likelihood We wish to evaluate (or maximize) the log\u2010evidence  </p> \\[ \\log p_{\\theta}(y) =\\log\\!\\int p_{\\theta}(y,z)\\,dz =\\log\\!\\int p_{\\theta}(y\\mid z)\\,p(z)\\,dz. \\] <p>2  Introducing a variational distribution Fix an arbitrary distribution \\(q_{\\phi}(z\\mid y)\\) that has the same support as \\(p(z\\mid y)\\).  Since \\(\\int q_{\\phi}(z\\mid y)\\,dz=1\\), we may write</p> \\[ \\log p_{\\theta}(y) =\\log\\!\\int q_{\\phi}(z\\mid y)\\,\\frac{p_{\\theta}(y,z)}{q_{\\phi}(z\\mid y)}\\,dz =\\log\\mathbb{E}_{z\\sim q_{\\phi}(\\cdot\\mid y)} \\biggl[\\frac{p_{\\theta}(y,z)}{q_{\\phi}(z\\mid y)}\\biggr]. \\] <p>3  Applying Jensen\u2019s inequality Since \\(\\log\\) is concave,  </p> \\[ \\log\\mathbb{E}_{q}\\bigl[X\\bigr] \\;\\ge\\; \\mathbb{E}_{q}\\bigl[\\log X\\bigr]. \\] <p>Setting \\(X = p_{\\theta}(y,z)/q_{\\phi}(z\\mid y)\\) gives the ELBO lower bound:</p> \\[ \\log p_{\\theta}(y) \\;\\ge\\; \\mathbb{E}_{z\\sim q_{\\phi}(z\\mid y)}\\! \\Bigl[\\log p_{\\theta}(y,z)\\;-\\;\\log q_{\\phi}(z\\mid y)\\Bigr] \\;=\\; \\mathcal{L}(\\theta,\\phi;y). \\] <p>4  Decomposing \\(\\mathcal{L}\\) Write</p> \\[ \\mathcal{L}(\\theta,\\phi;y) =\\mathbb{E}_{q_{\\phi}(z\\mid y)}\\bigl[\\log p_{\\theta}(y,z)\\bigr] \\;-\\;\\mathbb{E}_{q_{\\phi}(z\\mid y)}\\bigl[\\log q_{\\phi}(z\\mid y)\\bigr]. \\] <p>Since \\(\\log p_{\\theta}(y,z)=\\log p_{\\theta}(y\\mid z)+\\log p(z)\\), we get</p> \\[ \\mathcal{L}(\\theta,\\phi;y) =\\underbrace{\\mathbb{E}_{q_{\\phi}(z\\mid y)}\\bigl[\\log p_{\\theta}(y\\mid z)\\bigr]}_{\\text{(A) reconstruction}} \\;+\\; \\underbrace{\\mathbb{E}_{q_{\\phi}(z\\mid y)}\\bigl[\\log p(z)\\bigr] -\\mathbb{E}_{q_{\\phi}(z\\mid y)}\\bigl[\\log q_{\\phi}(z\\mid y)\\bigr]}_{\\text{(B) regularizer}}. \\] <p>But note that</p> \\[ \\mathbb{E}_{q}\\bigl[\\log p(z)\\bigr] -\\mathbb{E}_{q}\\bigl[\\log q(z)\\bigr] =-\\,D_{\\mathrm{KL}}\\bigl(q_{\\phi}(z\\mid y)\\,\\big\\|\\,p(z)\\bigr). \\] <p>Thus the usual form</p> \\[ \\boxed{ \\mathcal{L}(\\theta,\\phi;y) = \\mathbb{E}_{z\\sim q_{\\phi}(z\\mid y)}\\bigl[\\log p_{\\theta}(y\\mid z)\\bigr] \\;-\\; D_{\\mathrm{KL}}\\!\\bigl(q_{\\phi}(z\\mid y)\\,\\big\\|\\,p(z)\\bigr) }\\,. \\] <p>5  Exact decomposition with the true posterior One can also write exactly (no Jensen)</p> \\[ \\log p_{\\theta}(y) =\\mathcal{L}(\\theta,\\phi;y) \\;+\\; D_{\\mathrm{KL}}\\bigl(q_{\\phi}(z\\mid y)\\,\\big\\|\\,p_{\\theta}(z\\mid y)\\bigr), \\] <p>which shows \\(\\mathcal{L}\\) is a lower bound since \\(D_{\\rm KL}\\ge0\\).</p> <p>6  Extension to a dataset Given observations \\(\\{y_k\\}_{k=1}^m\\), assume they are i.i.d. under \\(p_{\\theta}\\).  The total ELBO is</p> \\[ \\mathcal{L}_{\\mathrm{total}}(\\theta,\\phi) =\\sum_{k=1}^m \\mathcal{L}(\\theta,\\phi;y_k) =\\sum_{k=1}^m \\Bigl\\{ \\mathbb{E}_{q_{\\phi}(z\\mid y_k)}[\\log p_{\\theta}(y_k\\mid z)] \\;-\\; D_{\\mathrm{KL}}(q_{\\phi}(z\\mid y_k)\\,\\|\\,p(z)) \\Bigr\\}. \\] <p>We then maximize \\(\\mathcal{L}_{\\mathrm{total}}\\) (or equivalently minimize its negative) w.r.t.\\ \\(\\theta,\\phi\\).</p> <p>This completes the formal derivation of the ELBO.</p>"},{"location":"chapter1/#measurements-y_k-are-iid","title":"Measurements \\(y_k\\) are i.i.d","text":"<p>The VAE theory just discussed implicitly assumes that the observations are drawn i.i.d from the data generating process. Is this correct?</p> <p>Answer</p> <p>Yes.  In the derivation and training objective we in fact assume that the dataset </p> \\[ \\{y_k\\}_{k=1}^m \\] <p>consists of independent, identically distributed draws from the underlying data\u2010generating law.  Concretely:</p> <p>1  Generative model factorization   We posit  </p> \\[       p_{\\theta}(y_{1:m},z_{1:m})       =\\prod_{k=1}^m p(z_k)\\,p_{\\theta}(y_k\\mid z_k), \\] <p>which implies the marginal over the observations factorizes as </p> \\[       p_{\\theta}(y_{1:m})       =\\prod_{k=1}^m p_{\\theta}(y_k). \\] <p>2  Variational posterior factorization     We choose an amortized variational family that likewise factorizes:  </p> \\[       q_{\\phi}(z_{1:m}\\mid y_{1:m})       =\\prod_{k=1}^m q_{\\phi}(z_k\\mid y_k). \\] <p>3  ELBO as a sum of per\u2010datum terms     Because of these factorizations, the total ELBO over the dataset decomposes into a sum of independent terms:  </p> \\[       \\mathcal L_{\\rm total}(\\theta,\\phi)       = \\sum_{k=1}^m         \\Bigl\\{           \\mathbb{E}_{q_\\phi(z_k\\mid y_k)}[\\log p_\\theta(y_k\\mid z_k)]           \\;-\\;           D_{\\mathrm{KL}}(q_\\phi(z_k\\mid y_k)\\,\\|\\,p(z_k))         \\Bigr\\}. \\] <p>Implicit in this decomposition is the assumption that each \\(y_k\\) is drawn i.i.d.\\ from the same marginal \\(p_\\theta(y)\\).</p> <p>If your data are not i.i.d.\\ (for instance time\u2010series, spatial fields, graphs, etc.), you must replace this vanilla VAE by a model that encodes the dependencies\u2014for example: \u2022 A sequential or recurrent VAE that conditions \\(p(y_k\\mid z_k)\\) on past observations \\(y_{&lt;k}\\). \u2022 A hierarchical or structured VAE that allows correlations among \\(z_k\\).  </p> <p>But the standard VAE as derived above does assume i.i.d.\\ observations.</p>"},{"location":"chapter2/","title":"Transformer VAE","text":""},{"location":"chapter2/#timedepth-series","title":"Time/Depth series","text":"<p>For time series observations, there is a concept called Dynamical VAE (DVAE). This encompasses many variants of VAE structures which allows for the latent variables \\(z_{1:m}\\) to be also a \u201ctime series\u201d. In essence then, the VAE becomes a sequence to sequence structure. What specific structures are applicable for this use case?</p> <p>Answer:</p> <p>Below is an overview of popular \u201cDynamical VAE\u201d (DVAE) variants that treat the latent variables \\(z_{1:m}\\) as a (stochastic) time series.  In these models, the VAE is extended to a sequence\u2010to\u2010sequence structure with explicit temporal dependencies in the latent space and/or in the decoder.</p> <p>1) Markovian or State\u2010Space DVAE</p> <p>\u2022 STORN (Bayer &amp; Osendorfer, 2014)   - \u201cStochastic Recurrent Networks.\u201d   - Prior over the latent sequence is a Markov chain:  </p> \\[       p(z_1)\\,\\prod_{k=2}^m p_\\theta(z_k\\mid z_{k-1}), \\] <p>with each \\(y_k\\) decoded from its latent \\(z_k\\).   - The encoder is typically an RNN that outputs approximate posteriors \\(q_\\phi(z_k\\mid z_{&lt;k},y_{1:k})\\).  </p> <p>\u2022 VRNN (Chung et al., 2015)   - \u201cA Recurrent Latent Variable Model for Sequential Data.\u201d   - Uses RNN hidden states \\(h_k\\) to condition both the prior and the approximate posterior of \\(z_k\\).   - Factorization:  </p> \\[       p_\\theta(y_{1:m},z_{1:m})       \\;=\\;\\prod_{k=1}^m p_\\theta(z_k\\mid h_{k-1})\\;p_\\theta(y_k\\mid z_k,h_{k-1}), \\] <p>with transitions in the hidden state \\(h_k\\).  </p> <p>\u2022 SRNN (Fraccaro et al., 2016)   - \u201cSequential Neural Models with Stochastic Layers.\u201d   - Also called the SSM-VAE (stochastic state\u2010space model).   - Explicit separation of deterministic RNN hidden state \\(h_k\\) and stochastic latents \\(z_k\\).   - The decoder is factorized as \\(y_k\\sim p_\\theta(y_k\\mid z_k,h_{k-1})\\).  </p> <p>In each of these, the latent dynamics \\(z_1,z_2,\\dots\\) form a Markov chain or partially observed Markov process, giving a sequence\u2010to\u2010sequence structure that captures temporal correlations.</p> <p>2) Non-Markov Latents with Attention or Convolution</p> <p>\u2022 Convolutional Temporal VAE   - Instead of an RNN, you can use 1D causal (or non\u2010causal) convolutions in the encoder and decoder to process sequences.   - Latent variables may still be factorized as \\(q_\\phi(z_{1:m}\\mid y_{1:m})=\\prod_k q_\\phi(z_k\\mid y_{1:m})\\), but each \\(z_k\\) is extracted by a local receptive field in time.  </p> <p>\u2022 Transformer\u2010based DVAE   - Replace RNNs by self\u2010attention (e.g., a Transformer) in both the prior/posterior networks.   - This can capture long\u2010range dependencies in the latent sequence without an explicit Markov assumption.  </p> <p>3) Continuous\u2010time DVAE</p> <p>\u2022 Latent ODE (Rubanova et al., 2019)   - Models latents as solutions to an ODE, so the latent trajectory \\(z(t)\\) evolves continuously in time.   - The VAE structure is maintained by having \\(z(t)\\) feed into a decoder that generates observations at discrete or irregular time points.   - Inference is done by \u201cencoding\u201d the observed data into initial conditions of the ODE with a recognition network.  </p> <p>\u2022 ODE2VAE (Y\u0131ld\u0131z et al., 2019)   - Combines an ODE model for a global latent trajectory with an additional recognition model for short\u2010term dynamics.  </p> <p>These approaches are particularly useful when your measurements \\(y\\) come in at irregular time intervals, or you want a physically motivated continuous\u2010time latent space.</p> <p>4) GP\u2010VAE and Other Gaussian\u2010Process Latent Structures</p> <p>\u2022 GP-VAE (Casale et al., 2018)   - Constrains \\(\\{z_k\\}\\) to come from a Gaussian Process prior so that  </p> \\[       p(z_{1:m})       = \\mathcal N(0,\\,K) \\] <p>for some temporal kernel \\(K_{t_i,t_j}\\).   - Then each \\(y_k\\) is decoded independently given \\(z_k\\).   - Posterior inference is done with structured variational approximations.  </p> <p>Such GP-based DVAEs capture smooth or band\u2010limited time correlations in the latent space.</p> <p>5) Hierarchical / Multi\u2010Scale DVAE</p> <p>\u2022 SHVRNN, HDVAE, etc.   - Introduce multiple latent layers or multi\u2010scale structure, e.g. a \u201cglobal\u201d latent that evolves slowly and local latents that capture finer time\u2010scale detail.   - Typically combine RNNs at each layer to handle separate timescales.  </p> <p>Implementation and ELBO</p> <p>Regardless of the chosen structure, training always proceeds by maximizing (a variant of) the sequential ELBO:</p> \\[ \\mathcal{L}_{\\mathrm{seq}}(\\theta,\\phi) =\\sum_{k=1}^m \\mathbb{E}_{q_\\phi(z_{k}\\mid z_{&lt;k},\\,y_{1:m})}   \\Bigl[\\log p_\\theta(y_k\\mid z_{\\le k})   \\Bigr] \\;-\\; \\sum_{k=1}^m   D_{\\mathrm{KL}}\\bigl(q_\\phi(z_k\\mid z_{&lt;k},\\,y_{1:m})     \\,\\|\\,p_\\theta(z_k\\mid z_{&lt;k})\\bigr), \\] <p>plus any additional terms for hierarchical or continuous\u2010time structure. The difference is in how you factorize: \u2022 the prior \\(p_\\theta(z_{1:m})\\), \u2022 the approximate posterior \\(q_\\phi(z_{1:m}\\mid y_{1:m})\\).  </p> <p>Summary</p> <p>To handle time series in a VAE:</p> <ol> <li> <p>Markov / RNN-based DVAE (STORN, VRNN, SRNN)    \u2013 Latent dynamics modeled by a hidden\u2010state transition.    \u2013 Encoder uses an RNN (or LSTM/GRU).</p> </li> <li> <p>Attention or Convolution-based DVAE    \u2013 A non\u2010Markov approach capturing long\u2010range or local correlations with self\u2010attention or causal convolutions.</p> </li> <li> <p>Continuous-time DVAE (Latent ODE, ODE2VAE)    \u2013 Latent trajectory follows an ODE; observations at irregular intervals.</p> </li> <li> <p>Gaussian-Process or MRF Prior    \u2013 Enforce smooth or band\u2010limited latent sequences via a GP kernel or pairwise Markov Random Field.</p> </li> <li> <p>Hierarchical / Multi\u2010scale DVAE    \u2013 Latents at multiple time scales, e.g. a \u201cglobal\u201d slow latent plus local fast latents.</p> </li> </ol> <p>All of these are \u201cDynamical VAEs,\u201d tailored to time\u2010series data by modeling the latent variables as a correlated or smoothly evolving sequence, rather than the default i.i.d. prior.</p>"},{"location":"chapter2/#transformer-as-encoder","title":"Transformer as Encoder","text":"<p>I want to use a transformer structure in encoder to model local and non-local correlations among the data \\(y_{1:m}\\). Recall that \\(y_k \\in \\mathbb{R}^n\\). The sequence length \\(m\\) is considered fixed.</p> <p>Let \\(l = 1, \\cdots, L\\) represent the layer index of a transformer block. Let the transformer block be modeled as a sequence to sequence map:</p> \\[ X_{l+1} = X_l + \\text{MHAttention}(X_l) \\] <p>Where \\(\\text{MHAttention}(\\cdot)\\) is a multi head attention block which operates on the sequence \\(X\\).</p> <p>How should such a multi head attention block be introduced in the encoder?</p> <p>Answer:</p> <p>Below is a step\u2010by\u2010step outline of how to introduce a multi\u2010head attention (MHA) block into your VAE encoder, focusing on the sequence of inputs \\(y_{1:m}\\) of length \\(m\\) (each \\(y_k \\in \\mathbb{R}^n\\)). We describe the typical Transformer encoder pipeline, which incorporates both MHA and a feed\u2010forward sublayer (often called a \u201cTransformer block\u201d).</p>"},{"location":"chapter2/#1-input-embedding-and-positional-encoding","title":"1) Input Embedding and Positional Encoding","text":"<p>1a) Embed each observation. Since each \\(y_k\\in\\mathbb{R}^n\\) is a vector (or could be an \\(n\\)-dimensional array), you first map it into a \\(d_{\\mathrm{model}}\\)-dimensional \u201ctoken\u201d:</p> \\[ X^{(0)}_k \\;=\\; W_{\\mathrm{emb}}\\,y_k \\;\\in\\;\\mathbb{R}^{d_{\\mathrm{model}}}\\quad \\text{for }k=1,\\dots,m. \\] <p>Stack these along the sequence dimension, forming  \\(\\;X^{(0)}\\in \\mathbb{R}^{m\\times d_{\\mathrm{model}}}\\).</p> <p>1b) Positional encoding. Self\u2010attention is permutation-invariant unless you add position information. A standard approach is to add a learnable or fixed positional embedding \\(P\\in\\mathbb{R}^{m\\times d_{\\mathrm{model}}}\\) to each token:</p> \\[ X^{(0)} \\;=\\; [X^{(0)}_1; \\dots; X^{(0)}_m] \\;\\mapsto\\; X^{(0)} + P \\;\\in\\;\\mathbb{R}^{m\\times d_{\\mathrm{model}}}. \\] <p>(You may store each row of \\(P\\) as \\(P_k\\), the encoding for position \\(k\\).)</p>"},{"location":"chapter2/#2-multihead-attention-and-residual","title":"2) Multi\u2010Head Attention and Residual","text":"<p>Let \\(X_l\\) be the sequence of hidden states at layer \\(l\\). A single multi\u2010head attention block (ignoring feed\u2010forward for the moment) maps</p> \\[ X_{l} \\;\\mapsto\\; X_{l}  \\;+\\; \\mathrm{MHAttention}(X_{l}). \\] <p>Below is how you compute \\(\\mathrm{MHAttention}(X_l)\\):</p> <p>2a) Compute Queries, Keys, Values. For each head \\(h=1,\\dots,H\\), define three linear maps:</p> \\[ W^Q_h,\\;W^K_h,\\;W^V_h  \\;\\in\\;\\mathbb{R}^{\\,d_{\\mathrm{model}}\\times d_{k}}. \\] <p>We form:</p> \\[ Q_h = X_l\\,W^Q_h,\\quad K_h = X_l\\,W^K_h,\\quad V_h = X_l\\,W^V_h. \\] <p>Here \\(Q_h,K_h,V_h \\in \\mathbb{R}^{m\\times d_k}\\). Typically we set \\(d_{\\mathrm{model}}=H\\times d_{k}\\).</p> <p>2b) Self\u2010Attention mechanism. For head \\(h\\), the output is</p> \\[ \\mathrm{Attn}_h(X_l) = \\mathrm{softmax}\\!\\Bigl(\\tfrac{Q_h\\,K_h^\\top}{\\sqrt{d_k}}\\Bigr) \\,V_h, \\quad \\in\\mathbb{R}^{m\\times d_k}. \\] <p>The \\(\\mathrm{softmax}\\) is applied row\u2010wise along the \u201csequence length\u201d dimension \\(m\\).</p> <p>2c) Concatenate heads. Stack these \\(H\\) outputs along the feature dimension to get</p> \\[ \\mathrm{ConcatHeads}(X_l) = \\mathrm{concat}\\!\\bigl[\\mathrm{Attn}_1(X_l),\\,\\dots,\\,\\mathrm{Attn}_H(X_l)\\bigr] \\;\\in\\;\\mathbb{R}^{m\\times (H\\,d_k)} =\\mathbb{R}^{m\\times d_{\\mathrm{model}}}. \\] <p>Then apply one more linear map \\(W^O\\in\\mathbb{R}^{d_{\\mathrm{model}}\\times d_{\\mathrm{model}}}\\) to form</p> \\[ \\mathrm{MHAttention}(X_l) = \\mathrm{ConcatHeads}(X_l)\\,W^O \\;\\in\\;\\mathbb{R}^{m\\times d_{\\mathrm{model}}}. \\] <p>2d) Residual connection and LayerNorm. In practice, the \u201cpre\u2010norm\u201d style is:</p> \\[ \\widetilde{X}_l = X_l + \\mathrm{MHAttention}\\bigl(\\mathrm{LayerNorm}(X_l)\\bigr). \\] <p>Then we pass \\(\\widetilde{X}_l\\) to a feed\u2010forward sublayer (next step in 3). (A \u201cpost\u2010norm\u201d style first adds the MHA output to \\(X_l\\) then applies \\(\\mathrm{LayerNorm}\\), either is widespread in practice.)</p>"},{"location":"chapter2/#3-feedforward-sublayer","title":"3) Feed\u2010Forward Sublayer","text":"<p>After multi\u2010head attention, each Transformer block also applies a position\u2010wise feed\u2010forward network \\(\\mathrm{FFN}\\).  Typically:</p> \\[ \\mathrm{FFN}(x) = \\max(0,\\;x\\,W_1 + b_1)\\;W_2 + b_2, \\] <p>all done \u201cper position\u201d in the sequence.  </p> <p>You then add another residual connection:</p> \\[ X_{l+1} = \\widetilde{X}_l + \\mathrm{FFN}\\!\\bigl(\\mathrm{LayerNorm}(\\widetilde{X}_l)\\bigr). \\] <p>Hence, summarizing the full layer \\(l\\):</p> \\[ \\begin{aligned} \\text{(1) } &amp;R_1 \\;=\\;\\mathrm{LayerNorm}(X_l), \\\\ &amp;\\widetilde{X}_l = X_l + \\mathrm{MHAttention}(R_1), \\\\ \\text{(2) } &amp;R_2 \\;=\\;\\mathrm{LayerNorm}(\\widetilde{X}_l), \\\\ &amp;X_{l+1} = \\widetilde{X}_l + \\mathrm{FFN}(R_2). \\end{aligned} \\]"},{"location":"chapter2/#4-stacking-l-layers","title":"4) Stacking L layers","text":"<p>You repeat Step 2 and Step 3 for \\(l=1,\\dots,L\\). The final output \\(X_{L}\\in\\mathbb{R}^{m\\times d_{\\mathrm{model}}}\\) is then interpreted as the encoder\u2019s representation for each of the \\(m\\) positions.</p>"},{"location":"chapter2/#5-mapping-to-latent-distributions","title":"5) Mapping to Latent Distributions","text":"<p>At the very end of your encoder, you produce (for each position in the sequence) the parameters of a Gaussian or discrete latent:  </p> \\[ \\mu_\\phi(y_{1:m})= \\mathrm{Linear}(X_{L}),\\quad \\log\\sigma_\\phi(y_{1:m})= \\mathrm{Linear}(X_{L}), \\] <p>giving you</p> \\[ q_{\\phi}\\bigl(z_{1:m}\\mid y_{1:m}\\bigr) = \\prod_{k=1}^m   \\mathcal{N}\\!\\bigl(z_k;\\,\\mu_{\\phi,k},\\,\\mathrm{diag}(\\sigma_{\\phi,k}^2)\\bigr). \\] <p>Then you reparameterize each \\(z_k\\) with \\(\\epsilon_k\\sim\\mathcal{N}(0,I)\\) to feed into the decoder.</p>"},{"location":"chapter2/#summary","title":"Summary","text":"<p>Thus, to incorporate multi\u2010head self\u2010attention in the encoder:</p> <p>\u2022 (a) Embed each input \\(y_k\\) into a common feature dimension and add positional encodings. \u2022 (b) Pass the resulting \\(\\{X_l\\}\\) through \\(L\\) Transformer blocks, each containing (i) multi\u2010head self\u2010attention with a residual connection, and (ii) a feed\u2010forward sublayer with another residual. \u2022 (c) Output \\(\\mu_\\phi,\\sigma_\\phi\\) (or discrete logits) for each \\(z_k\\).  </p> <p>This yields an encoder structure that can capture both local and long\u2010range correlations along the sequence \\(\\{y_k\\}_{k=1}^m\\).</p>"},{"location":"chapter2/#transformer-encoder-output","title":"Transformer encoder output","text":"<p>So, as the sequence \\(\\{y_k\\}_1^m\\) is input into the encoder, it goes through the transformer block and the output is a sequence \\((\\mu_{\\phi}, \\sigma_{\\phi})_{1:m}\\). Is this correct?</p> <p>Answer:</p> <p>Yes, exactly.  In a typical \u201csequence-to-sequence\u201d VAE encoder based on the Transformer, you do indeed end up with a per\u2010position Gaussian (or other distribution) over the latent variables.  That is,</p> <ol> <li> <p>You feed the entire sequence \\(y_{1:m}\\) (after embedding + positional encoding) into \\(L\\) layers of multi\u2010head attention and feed\u2010forward blocks.</p> </li> <li> <p>The Transformer\u2019s final output is another sequence of the same length, call it </p> </li> </ol> \\[ X^{(L)}  = \\bigl\\{\\;x_1^{(L)},\\;x_2^{(L)},\\;\\dots,\\;x_m^{(L)}\\bigr\\}, \\] <p>where each \\(x_k^{(L)}\\in \\mathbb{R}^{d_{\\mathrm{model}}}\\).</p> <ol> <li>You then apply linear heads (or 1\u00d71 convolutions if you prefer) to map each \\(x_k^{(L)}\\) into the mean and log\u2010variance of a Gaussian, i.e. </li> </ol> \\[ (\\mu_{\\phi,k},\\;\\log\\sigma_{\\phi,k})  = \\mathrm{Linear}\\bigl(x_k^{(L)}\\bigr).   \\] <p>Thus for each position \\(k\\), your encoder outputs  </p> \\[ \\mu_{\\phi,k},\\; \\sigma_{\\phi,k} \\;\\in\\;\\mathbb{R}^{d}, \\] <p>defining the approximate posterior distribution </p> \\[ q_\\phi\\bigl(z_k\\mid y_{1:m}\\bigr) = \\mathcal{N}\\!\\bigl(z_k;\\,\\mu_{\\phi,k},\\,\\mathrm{diag}(\\sigma_{\\phi,k}^2)\\bigr). \\] <p>Hence the final result really is a sequence of mean\u2013log\u2010variance pairs \\(\\{(\\mu_{\\phi,k}, \\sigma_{\\phi,k})\\}_{k=1}^m\\).</p>"},{"location":"chapter2/#the-rest-of-vae-theory-holds","title":"The rest of VAE theory holds","text":"<p>And then the rest of the VAE theory and calculations proceed for each \\(k\\) as before?</p> <p>Answer</p> <p>Yes, precisely. Once you have obtained  </p> \\[ \\{\\mu_{\\phi,k},\\,\\sigma_{\\phi,k}\\}_{k=1}^m, \\] <p>each pair describes a latent posterior for position \\(k\\) via  </p> \\[ q_{\\phi}\\bigl(z_k \\mid y_{1:m}\\bigr) =\\mathcal{N}\\!\\bigl(z_k;\\,\\mu_{\\phi,k},\\;\\mathrm{diag}(\\sigma_{\\phi,k}^2)\\bigr). \\] <p>From there, the remainder of the VAE machinery applies exactly as before:</p> <p>1) Reparameterize    For each \\(k\\), you draw  </p> \\[      z_k      = \\mu_{\\phi,k} + \\sigma_{\\phi,k}\\,\\odot\\,\\epsilon_k,      \\quad      \\epsilon_k\\sim\\mathcal{N}(0,I_d). \\] <p>2) Decode    You pass \\(\\{z_k\\}_{k=1}^m\\) into your decoder to obtain  </p> \\[      p_{\\theta}\\bigl(y_k \\mid z_k\\bigr)      \\quad      (\\text{or possibly } p_{\\theta}\\bigl(y_{k} \\mid z_{1:m}\\bigr)\\text{, depending on your chosen factorization}). \\] <p>3) ELBO    You then form the usual sum of reconstruction minus KL terms. For instance, if you assume factorized priors on \\(\\{z_k\\}\\), the ELBO is  </p> \\[      \\mathcal{L}(\\theta,\\phi)      = \\sum_{k=1}^m      \\Bigl\\{        \\mathbb{E}_{q_{\\phi}(z_k\\mid y_{1:m})}        \\bigl[\\log p_{\\theta}(y_k \\mid z_k)\\bigr]        \\;-\\;        D_{\\mathrm{KL}}\\!\\bigl(q_{\\phi}(z_k\\mid y_{1:m})              \\,\\big\\|\\,              p(z_k)\\bigr)      \\Bigr\\}. \\] <p>(Of course, if you opted for a Markov or correlated prior among \\(\\{z_k\\}\\), you would just update the KL term accordingly.)</p> <p>In short: each position \\(k\\) gets its own latent distribution from the encoder\u2019s Transformer output, and everything else\u2014sampling \\(z_k\\), decoding, and summing into the ELBO\u2014proceeds just as in the usual VAE framework, only repeated across the sequence index \\(k\\).</p>"},{"location":"chapter2/#decoder-also-as-a-transformer","title":"Decoder also as a transformer","text":"<p>In principle, you could replace the decoder as well with a transformer structure. Is this possible?</p> <p>Answer:</p> <p>Yes, absolutely.  Just as you used a Transformer encoder for modeling correlations in the input sequence, you can similarly replace your decoder network with a Transformer.  In that case, after sampling the latents \\(\\{z_k\\}_{k=1}^m\\), you feed them into a Transformer\u2010based decoder network that outputs parameters of the likelihood for \\(\\{y_k\\}\\).  Below is a high\u2010level outline.</p> <p>1) Latent sequence as decoder input    After you sample  </p> \\[      z_k \\;=\\;\\mu_{\\phi,k} + \\sigma_{\\phi,k}\\odot \\epsilon_k,\\quad      \\epsilon_k\\sim \\mathcal{N}(0,I_d), \\] <p>for each \\(k\\), you form a sequence \\(Z = \\{z_1,z_2,\\dots,z_m\\}\\).  Alternatively, you might downsample \\(\\{z_k\\}\\) or use a single global latent if your architecture differs.  </p> <p>2) Option A: direct feed into a Transformer decoder    In the simplest approach, treat \\(\\{z_k\\}\\) as the \u201ctokens\u201d for the decoder, embed each \\(z_k\\) into a dimension \\(d_{\\mathrm{model}}\\), add positional encoding, and apply the usual multi\u2010head self\u2010attention + feed\u2010forward layers to produce a sequence of outputs </p> \\[      X_{\\mathrm{dec}} = \\mathrm{TransformerDecoder}(Z). \\] <p>Then a final linear head can produce \\(\\{\\mu_{\\theta,k},\\sigma_{\\theta,k}\\}\\) for \\(p_{\\theta}(y_k\\mid z_{1:m})\\) (or for a factorized version \\(p_{\\theta}(y_k\\mid z_k)\\), depending on your modeling choice).</p> <p>3) Option B: cross\u2010attention decoder    If your decoding process benefits from \u201ccross\u2010attention\u201d to the latents, you can adopt the common \u201cencoder\u2013decoder\u201d Transformer pattern:    \u2022 The latent sequence \\(Z\\) passes through a purely feed\u2010forward or self\u2010attention \u201cencoder\u201d, producing some representation \\(H = \\mathrm{Enc}(Z)\\).    \u2022 The decoder receives some query positions (e.g., an index for each output location \\(k\\)), and uses cross\u2010attention over \\(H\\) to produce the likelihood parameters for \\(y_k\\).    This is analogous to standard Transformers in machine translation, except now the \u201cinput sentence\u201d is your latent code sequence \\(Z\\).</p> <p>4) Autoregressive vs. non\u2010autoregressive decoding    \u2022 If you want a purely parallel reconstruction of \\(\\{y_k\\}\\), the decoder can be non\u2010autoregressive and simply produce \\(\\bigl(\\mu_{\\theta,k},\\sigma_{\\theta,k}\\bigr)\\) for each element in one shot.    \u2022 If you prefer an autoregressive model for each coordinate or token of \\(y_k\\), you could implement a Transformer with causal masking on the output side, letting it attend left\u2010to\u2010right (though that\u2019s more common in purely \u201cTransformer language models\u201d).</p> <p>5) ELBO stays the same    Regardless of the decoder\u2019s architecture, the ELBO expression remains</p> \\[      \\sum_{k=1}^m        \\mathbb{E}_{q_{\\phi}(z_k\\mid y_{1:m})}\\bigl[\\log p_{\\theta}(y_k\\mid z_{1:m})\\bigr]      \\;-\\;      \\sum_{k=1}^m        D_{\\mathrm{KL}}\\bigl(q_\\phi(z_k\\mid y_{1:m}) \\,\\|\\, p(z_k)\\bigr), \\] <p>if your prior factorizes.  Of course, if the decoder condition is \\(\\{z_k\\}_{k=1}^m\\) in some particular attention structure, you just code that into \\(\\log p_{\\theta}(y_k\\mid z_{1:m})\\).</p> <p>6) Summary    \u2022 Yes, you can replace the decoder with a Transformer in the exact same way you replaced the encoder.    \u2022 You either feed your latent sequence into a self\u2010attention stack or do a more sophisticated \u201cencoder\u2013decoder\u201d cross\u2010attention, depending on your generative design.    \u2022 The rest of the VAE pipeline\u2014reparameterizing the latents, computing the reconstruction log\u2010likelihood, summing up the KL terms\u2014remains unchanged in principle.</p>"},{"location":"chapter2/#differentiability-with-transformers","title":"Differentiability with transformers","text":"<p>The training of the VAE architecture requires differentiability in the chosen architecture. Using transformers as encoders and maybe even the decoder does not affect the differentiability?</p> <p>Answer:</p> <p>Precisely\u2014multi\u2010head self\u2010attention, feed\u2010forward layers, and other standard Transformer components are all built from differentiable operations (i.e., matrix multiplications, softmax, layer normalizations, etc.).  Consequently:</p> <p>1) Transformers are differentiable as feed\u2010forward blocks.    \u2212 Each sub\u2010operation (linear projections for queries/keys/values, softmax, residual connections, etc.) is continuous and differentiable w.r.t. its parameters.    \u2212 Autograd frameworks (TensorFlow, PyTorch, JAX, etc.) can backpropagate through them just like through MLPs or convolutions.</p> <p>2) The reparameterization trick remains the same.    \u2212 You sample the latent variable(s) z using the usual      z = \u03bc + \u03c3 \u2299 \u03b5, \u03b5 \u223c \ud835\udca9(0, I).    \u2212 This injects noise into z in a way that still allows backprop to pass through \u03bc and \u03c3.    \u2212 Whether \u03bc and \u03c3 come from an MLP, convolution, or Transformer is immaterial; the gradient flows without problem.</p> <p>3) The ELBO\u2019s reconstruction term is still differentiable.    \u2212 The log p\u03b8(y | z) is likewise a feed\u2010forward pass (in this case, the decoder).    \u2212 If you choose a Transformer decoder, it, too, is composed of differentiable layers that can be optimized via backprop.</p> <p>Hence using Transformers does not break or hamper differentiability.  You can train a \u201cTransformer\u2010VAE\u201d end\u2010to\u2010end exactly the same way as with other neural architectures, leveraging any standard automatic differentiation software.</p>"}]}